\documentclass[./main.tex]{subfiles}
\begin{document}

Below, I provide explanations of how I have applied various habits of Mind and Foundational Concepts in this work. The explanations are divided into four sections.

\begin{enumerate}
    \item Section \ref{hc:section-analytical} describes the HCs applied in constructing the theoretical and analytical content in this work. Largely the early chapters, which establish the theoretical foundation and guide the design of the core contribution.
    
    \item Section \ref{hc:section-technical} describes the HCs applied in designing and constructing the technical components of the work. These are focused on statistics, code implementation, and experimental validation.
    
    \item Section \ref{hc:section-writing} describes the HCs applied to the writing and structuring of the paper, a level above the content itself.
    
    \item Section \ref{hc:section-process} describes HCs applied to the meta-process of completing this project.
\end{enumerate}

\section{Analytical HC Applications}
\label{hc:section-analytical}

\subsection*{\textbf{\#critique}}
\label{hc:critique}

This work is based on a critique of the benchmarking methods used in the causal inference literature. In Chapter \ref{chap:litreview}, I review a wide set of papers - both those which explicitly set out to introduce novel benchmarking approaches and those which implicitly introduce approaches by using them to test new causal estimators. I distill this landscape of benchmarking approaches into \textit{Pure} and \textit{Hybrid} classes (and empirical/synthetic sub-classes) and discuss the relative advantages and disadvantages of the different approaches in each class. This discussion is based on eight properties that elucidate the key differences between approaches. The writing in this chapter demonstrates the ability to both engage deeply with individual works - to understand the methods employed/proposed - and to synthesize many related works in order to extract insights from the literature as a whole. The critique in Chapter \ref{chap:litreview} is enabled by an application of \nameref{hc:breakitdown}, which is used to simplify the problem of comparing the vast array of different benchmarking approaches. The comparison then provides the background required for the \nameref{hc:gapanalysis} that guides the design of Maccabee.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#breakitdown}}
\label{hc:breakitdown}

The primary purpose of the Literature Review (Chapter \ref{chap:litreview}) is to compare and contrast the methods of causal inference benchmarking from the literature. Analyzing which method is `best' is a fairly daunting problem, but one which is made easier by the extensive application of \nameref{hc:breakitdown}. I first breakdown the concept of quality in two axes of validity - specific and general validity - and then express these axes in terms of eight validity properties (with validity on each axis influenced by multiple properties). This makes it much easier to compare different methods by simply determining the value of the eight properties and then comparing property values across methods. This can be thought of as breaking up the comparison into a series of smaller comparisons that add up to a comparison of the overall quality of the various methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#gapanalysis}}
\label{hc:gapanalysis}

This paper is driven by a gap analysis at two levels. At the broad-approach level, the Literature Review (Chapter \ref{chap:litreview}) identifies that the vast majority of evaluation approaches used in practice fall into the class of \textit{pure} methods - sub-optimally trading off \textit{specific} and \textit{general} validity. The solution to this gap is found in the literature in the form of \textit{hybrid} approaches. The approaches make a better trade-off between realism, bias, and control of the distributional setting of the data generating process. However, as established in Chapter \ref{chap:macdesign}, even within the class of \textit{hybrid} methods, a gap remains - partly in the design space and partly in the implementation space. There is no general-purpose package/tool available for researchers to apply hybrid benchmarking to their own causal inference methods. Code that is provided, for example by \textcite{Dorie2019Automated1}, is parochial - fixed to specific data and parameterization. This means that enormous potential is left unrealized - the power of sample-based hybrid benchmarking is that it can be used to target method-specific distributional settings and applied to different empirical data. Maccabee - the core deliverable in this work - is designed to address this gap in the solution space. It provides a tool that allows researchers to easily apply hybrid benchmarks to their own methods, using their own data, and targeting easily-specified locations in the distributional problem space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#evidencebased}}
\label{hc:evidencebased}

I have invested substantial effort into collecting, reviewing, and presenting a wide body of evidence to provide a foundation for my work. I have spent a lot of time thinking about how to organize this evidence into a clear and cohesive set of chapters that build on each other to motivate and explain my core contribution. The introduction in Chapter \ref{chap:intro} connects the work in this paper to the broader set of related predictive and causal inference literature. It does this by citing numerous high-quality review papers and extracting/synthesizing key connections between them and the specific field of causal inference benchmarking. Chapter \ref{chap:framework} establishes the theoretical foundation of the work by connecting and reviewing the two leading frameworks for causal inference - Rubin's Potential Outcomes and Pearl's Structural Causal Models. Chapter \ref{chap:litreview}, as discussed above, provides a close review of a wide (but not exhaustive) set of the causal inference benchmarking literature.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#sourcequality}}
\label{hc:sourcequality}

The tag for \nameref{hc:evidencebased} above says much of what needs to be said for this tag. Here, I emphasize that I have exclusively used published, peer-reviewed papers - often relying on seminal works with hundreds or thousands of citations (a rough proxy for reliability and quality). Peer-review does not guarantee quality and, indeed, I have critiqued many of the ideas proposed. But, as a minimum bar, it does ensure that other independent experts have read the works cited and found them suitable for publication and broader consumption.

\vspace{\baselineskip}

It is also worth noting that the sources which appear in this work represent only about a third of the sources which I reviewed during the past year of research on this topic. This means the sources used represent a strict filtering for articles that are most `relevant' to my analysis. The definition of relevance varies chapter to chapter. For example, for sources in Chapter \ref{chap:framework}, I prioritized explanatory clarity and status as seminal work that established the core tenants of the field of causal inference. For sources in Chapter \ref{chap:litreview}, I prioritized the quality and popularity of the benchmarking approach in order to ensure the review included the best and most widely used methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Technical HC Applications}
\label{hc:section-technical}

\subsection*{\textbf{\#algorithms}}
\label{hc:algorithms}

Algorithms and algorithmic thinking are applied at a number of levels in this work. At a high level, the core operational flow of Maccabee can be thought of as the implementation of an algorithm for sampling from the statistical model of the Maccabee benchmark formalized in Chapter \ref{chap:macdesign}. This sampling requires (repeatedly) sampling treatment/outcome functions, applying these functions to empirical data, fitting estimators, and calculating metrics. Executing these steps correctly and efficiently required the design of an execution flow that satisfies the dependencies of the various steps while minimizing wasted computation. Two examples of important algorithmic design principles:

\begin{enumerate}
    \item Caching intermediate results where possible:
    
    \begin{itemize}
        \item Source empirical data and all sampling parameters (including calculated parameters) are loaded and calculated once at the start of execution and then cached for use through DGP and data sampling.
        
        \item During DGP sampling, all values which can be preserved across samples are calculated once and then cached. This includes memoization of expensive, low-level functions like the calculation of all combinations of covariates that could be used to instantiate each nonlinear transform.
        
        \item During Data sampling, all data components that are constant across data samples are calculated once and then cached. This is supported by the use of a DSL which allows users to specify which aspects of the DGP should be cached between runs.
        
    \end{itemize}
    
    Across all these examples, the implementation of caching required a design with proper separation of concerns and abstraction. This enables the caching logic to be hidden from downstream consumers, greatly reducing complexity. See \nameref{lo:CodeDesign} for more on how caching was implemented through the careful design of the package components and interfaces.
    
    \item Parallelization: where tasks are not serially dependent, the code is constructed with parallel execution in mind. DGP sampling, data sampling, model fitting, and metric calculation are implemented such that many samples can be taken in parallel with eventual results merged for analysis when required. The implementation of this required careful thought and optimization. For example, hundreds to thousands of data set samples are taken from each DGP, and each data set is a non-trivial object in terms of memory footprint (thousands of values). This means that the communication overhead incurred during each transfer of tasks and data back and forth between the main process and worker processes can become substantial. Two steps are taken to minimize this overhead. Firstly, \textit{co-locating} computation across processes. Rather than parallelizing each parallelizable task - DGP sampling, data sampling, model fitting, metric calculation - separately, these tasks are executed in parallel but in the same process. This means that large objects are not transmitted back and forth between the main process and worker processes - only small result values are. There is more nuance to this because one needs to ensure all CPU resources are properly utilized. So, at times, it makes sense not to co-locate to ensure a sufficient degree of parallelization is possible. This motivates a modular parallelization scheme that can be used differently in different benchmarks. Second, \textit{chunking}: multiple tasks and associated data are sent to workers in batches to minimize the number of communication events, each of which incurs overhead. Taken together, these two optimization steps reduced processing time for the same benchmark by around 50\% with the same CPU resources.
\end{enumerate}
 
At a lower level, there are numerous algorithmic optimizations throughout the package that are crucial to the speed of execution. These largely come down to using data structures and packages that are conducive to the efficient execution of the operations being performed. For example:

\begin{enumerate}
    \item In the DGP sampling implementation, Python set objects are used to find and adjust the overlap between the sampled covariates for the treatment and outcome functions in optimal time. See the optimality explanations \href{https://wiki.python.org/moin/TimeComplexity}{here}.
    
    \item In the data sampling implementation, sampled DGP functions are converted to numpy functions that can be applied to all observations as arrays rather than repeatedly invoking the function for each observation. These functions are defined over all the covariates in the data set rather than those covariates known to appear in the function to avoid the need to index into the covariate data structure to extract the right arguments. Just these two simple changes yielded a performance improvement of ~70\% relative to naive implementation.
    
    \item The sampled DGP functions, mentioned above, can be compiled and loaded as Python C extensions using autogenerated C code. This approximately halves the execution time per observation at the cost of around ~2 seconds of compilation time per sampled DGP (these values vary with the number of covariates, see below). If the number of data samples drawn from the DGP is large enough, then the amortized cost of this compilation combined with faster sampling produces on overall speed up. The advantage of the compiled code over the non-compiled code grows with the number of covariates (shrinking the number of samples required to justify the compilation time). This is because the execution time of the non-compiled code scales as $O(N^C)$ with the number of covariates $N$ (and an unknown polynomial degree $C$), while the compilation and execution time of compiled code scales as $O(N)$. See the \href{\RTDurl/advanced/parallelization.html}{documentation on compilation} for more detail and experimental results.
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#variables}}
\label{hc:variables}

The \#variables HC is applied extensively throughout this paper. I'd like to highlight three related but conceptually distinct places in which variables are used to construct formal descriptions of complex, inter-related systems.

\begin{enumerate}
    \item Chapter \ref{chap:framework} establishes the theoretical framework for causal inference. This framework is premised on the recognition of three variable types - outcome (dependent) variables, treatment (independent) variables, and confounding variables. By expressing the relationship between these three variable types - using either conditional expectation (in the Potential Outcomes framework) or graphical models (in Structural Causal Modeling) - it is possible to produce formal descriptions of causal effects in (partially) observed systems and, thus, to formally identify the sources of bias in causal inference from observational data. The formal description of the observed data takes the form of a joint distribution defined over the variables of various types. This connects to the discussion of \nameref{hc:distributions} below.
    
    \item Chapter \ref{chap:problemspace} establishes the distributional problem space. This chapter is premised the formal descriptions of the relations between variables outlined in Chapter \ref{chap:framework}. It extends the analysis in that chapter to show how different instantiations of the joint distribution over the variables result in different \textit{distributional problem settings}, with different resultant performance for different methods of observational causal inference. This chapter establishes the (hopefully spanning) space of possible joint distributions and the effect of the position of systems along the axes of the space on the ability of causal inference estimators to recover true causal effects. The analysis in the chapter demonstrates the power of formal descriptions in helping to develop theoretical new analysis/problem descriptions.
    
    \item Finally, Chapter \ref{chap:macdesign} (section \ref{mac:formalspec}) extends the formal description of observational systems from Chapter \ref{chap:framework} into a complete, formal description of the benchmarking approach proposed in this paper. This formalization describes estimator quality metrics as dependent variables arising from the combination of the observed system (made up of the variables described above, sampled from sampling distributions) and the causal estimand produced by the (independent) choice of causal estimator. In this framing, the benchmark can (rightly) be thought of as an experiment that evaluates which (independent) choice of estimator produces the best (dependent) quality result in a (controlled) distributional setting. This is a nice way of understanding and expressing the value of hybrid Monte Carlo benchmarking - it allows for control over the `variables' (observed system distributional setting) that `confound' the dependent, measured performance of the independently selected causal estimator. Thus, allowing for valid experimental evidence of estimator performance.
    
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#distributions}}
\label{hc:distributions}

In each of the three cases described in \nameref{hc:variables} above, the formal model of the relationship between the observed/unobserved variables takes the form of a joint distribution defined over the variables. This framing as a joint distribution is implicit and relatively unimportant in Chapter \ref{chap:framework}. It is of much greater importance in Chapters \ref{chap:problemspace} and \ref{chap:macdesign} as these chapters establish the formal theory that underpins Maccabee's sample-based benchmarks. The understanding of the benchmark as a sample from the joint distribution over all of the variables (data, estimand, metrics) is the motivation behind all Monte Carlo-based benchmarking.

\vspace{\baselineskip}

Beyond this theoretical application, distributions are also used throughout the Maccabee code. The \textit{DGP Sampling Parameters} - which control the region from which DGPs are sampled - work by parameterizing a series of fairly simple distributions that work together, as described in Chapter \ref{chap:macdesign}, to control the distributional setting of sampled DGPs. The complex part of implementing this was not in the parameterization of each component distribution but in coordinating them to produce DGPs with the desired properties. For example, the sampled value of the outcome noise needs to be scaled relative to the outcome and treatment effect function coefficients (and covariates) so that the noise doesn't overwhelm the effect. This co-ordination, and other similar examples, are partially automated through normalization, partially pre-configured through hand-tuned parameterization, and partially left up to user-specified tuning of the \textit{DGP sampling parameters}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#sampling}}
\label{hc:sampling}

Sampling is at the heart of this work - both in identifying the problem with existing approaches and constructing a solution. The problem with existing, \textit{pure} approaches to benchmarking - whether empirical or synthetic - is that they are effectively a single sample from the distribution over performance in a region of the problem space (Chapter \ref{chap:macdesign}). In synthetic approaches, there is good reason to believe this sample may be biased toward samples in which estimators are likely to perform better. In empirical studies, it is unclear which region the sample is drawn from and whether it is valid. Even without these problems, a single sample is not necessarily representative of the distribution of estimator performance (which has intrinsic variability, even in controlled settings - see Chapter \ref{chap:macvalidation}).

\vspace{\baselineskip}

The solution to this problem is a mechanism that can be used to construct a representative sample of estimator performance from a known/controlled region of the distributional problem space. Maccabee is designed to provide just such a mechanism. Chapter \ref{chap:problemspace} establishes the domain from which data problem instances can be sampled. Chapter \ref{chap:macdesign} shows how Maccabee can be used to draw (unbiased) data samples from controlled regions of this domain. It further shows how these data samples can be used to produce samples of estimator performance metrics. These sampled metric values are key to the valid and robust evaluation of causal estimator performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#descriptivestats}}
\label{hc:descriptivestats}

Maccabee benchmarks are based on Monte Carlo sampling. This means they result in a sample of the distribution over various measurable quantities, referred to as metrics in this paper. Descriptive statistics are to summarize these distributions. It is worth drawing a distinction between the two primary ways in which descriptive statistics are used in the benchmarking process:

\begin{enumerate}
    \item The performance of a causal estimator on each data set is measured using performance metrics. The overall performance of an estimator is, by default, summarized using the mean of the sampled performance metric values. Although, depending on the application, alternate summary statistics, like the median, percentile values, or inter-quartile range, may be useful. The raw performance samples are provided so that users can calculate their own summary statistics. Because the primary use of the benchmark is to compare the performance of different methods, the overlap between the performance metric distribution of different methods is of particular concern. By default, the package reports standard deviations on the performance metric values as a way to roughly quantify the variability of performance and the overlap between estimator performance distributions. If the difference between the mean performance of different metrics is much larger than the standard deviation, then this is a good indication of a true difference in underlying performance. This should be used with caution. The true distribution over the performance metrics is unknown, so formal tests of significant difference - which generally rely on known sampling distributions - cannot be applied. Maccabee provides all the information required to perform alternate comparisons - for example, bootstrapped confidence intervals - but the choice and implementation is left up to users.
    
    \vspace{\baselineskip}
    
    Great care was taken in designing the sampling and performance metric calculation procedure to ensure that different sources of variance could both be disambiguated and properly averaged over. The estimator performance metrics are each defined over the sampling distribution of the underlying estimator (see Chapters \ref{chap:intro} and \ref{chap:macimplementation}). So, a single estimate of each performance metric value requires sampling and aggregating multiple estimand samples, each from a sampled data set. In order to capture and quantify the variance of this aggregation, performance metrics are calculated across a user-specified number of data sampling \textit{runs}, each comprising a user-specified number of data samples. Given that all data samples are drawn from the same DGP, we would expect fairly low variance between sampling runs. This means that if substantial variance is observed across runs, then the number of samples per run should be increased (or the number of runs should be increased such that the standard error of the mean across runs is reduced). The performance metric value for each sampled DGP is then the mean of the performance metric for each run, which is reported alongside a mean run-level standard deviation (not the same as the standard error of the DGP mean estimate). Finally, the performance in the distributional setting (defined by a set of sampling parameters) is the mean over the average performance metric of a user-specified number of sampled DGPs. This is the aggregation step that is unique to Maccabee, given its ability to sample DGPs from a region in problem space. The variation at this level may be fairly large, given that an estimator may interact quite differently with different DGPs from a similar region of problem space.
    
    \item  The performance metrics discussed above are complemented by a set of data metrics that are designed to measure the location of the sampled DGPs in the distributional problem space. These metrics are introduced in Chapter \ref{chap:problemspace}. The simpler nature of these metrics results in a simpler scheme for calculating summary statistics. Data metric values are averaged over all data samples for each DGP and then averaged over DGPs in a distributional setting (defined by a set of sampling parameters). 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#probability}}
\label{hc:probability}

Formal probability theory is used throughout Chapter \ref{chap:framework} to establish the notation and theory behind the two frameworks for causal inference. The analysis in the chapter synthesizes two frameworks that come from different schools of statistical thought. Rubin's Potential Outcomes framework is Frequentist in nature, defining estimators and estimands over fixed samples from populations. Causal estimands are expressed as conditional expectation values over the sample data. Pearl's Structural Causal Modeling (SCM) is Bayesian in nature, expressing estimands as the parameters of the model that relates the various variables in the problem.

\vspace{\baselineskip}

The analysis in the chapter borrows the best expositional components from both frameworks to produce a single, synthesized explanation of observational causal inference. For example, I use a simple SCM graphical model of an outcome, treatment assignment, and a single confounding variable to explain why the conditioning (on all covariates) in Rubin's expected outcomes results in an unbiased estimate of the true effect of the treatment assignment. And, why this process is functionally equivalent to Pearl's backdoor criterion for causal effect identification. This kind of synthesis demonstrates a strong grasp of both the quantitative/mathematical mechanics of both frameworks as well as the deeper, more philosophical similarities and differences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#correlation}}
\label{hc:correlation}

Chapter \ref{chap:intro} first introduces the distinction between predictive, phenomenological correlation and fundamental, mechanistic correlation - which is how I describe causation in this paper. This is a somewhat unusual framing. One is used to hearing that \textit{correlation is not causation}. But, while this statement is correct as a simple heuristic, it disguises nuance which helps to understand and operationalize causal inference:

\vspace{\baselineskip}

Chapter \ref{chap:framework} explains the distinction between the levels of correlation in more detail - there, it is established that phenomenological correlation includes multiple mechanistic \textit{pathways} of influence/dependence between the treatment and outcome variables and is, thus, unable to predict the results of intervention which sever some of these pathways. In this paradigm, inferring causal effects requires inferring the direct, mechanistic influence of the treatment on the outcome. The question then becomes why this mechanistic influence should be described/analyzed in terms of correlation. The answer to this is obvious but disguised by the pedagogical simplification of causal mechanisms. In most introductory texts - including Chapter \ref{chap:framework} - treatment effects are described as binary (treated/untreated) and homogeneous (the same for all individuals, noiseless). In reality, even after perfectly controlling for all confounding, the relationship between treatment and outcome can be non-trivial. If treatment is continuous, and/or there is intrinsic noise in the effect mechanism, then the true treatment-outcome mechanism cannot be described by a constant effect. Rather, it is described by a (potentially complex) stochastic function. This means that, much like for any other pair of variables related by a stochastic function, the concept of correlation is directly applicable to describing the causal mechanism. It is worth explicitly stating that this doesn't mean naive correlation analysis is equivalent to causal inference. Most of causal inference research is dedicated to working out methods to accurately recover the functional form and parameters of the treatment-outcome mechanism. But all of these approaches essentially aim to perform fundamental, mechanistic correlation analysis.

\vspace{\baselineskip}

Shifting gears, standard correlation analysis is used in a more practical/applied sense in a few places in this work. The most interesting application is in constructing metrics to quantify the position of sampled data in the distributional problem space. For example, the non-linearity of the outcome/treatment mechanisms is quantified by measuring the $R^2$ value of a linear regression of the outcome/treatment status onto the observed covariates. This implicitly uses the logic that a weaker \textit{correlation} between the linear covariates (in the input space of the mechanism) and the outcome/treatment (in the output space of the mechanism), implies a more non-linear functional relation. Correlation is also used in the output space of the treatment and outcome mechanisms to quantify their \textit{alignment} - the similarity of their inputs and functional forms (see Chapter \ref{chap:problemspace}). Here, the $R^2$ value of a regression between the treatment assignment and outcome is used to measure the similarity of the inputs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#testability}}
\label{hc:testability}

One of the key challenges in this work was designing a way to validate Maccabee's benchmarking approach and implementation. Validation is required for two aspects of the design and implementation:

\begin{enumerate}
    \item \textbf{Correctness:} refers to whether the design and implementation of Maccabee achieve its design goal: to provide the ability to sample DGPs from specified regions of the distributional problem space and then perform (valid) benchmarks using these DGPs.
    
    \item \textbf{Superiority:} refers to whether the design and implementation are superior to existing benchmarking methods (the theoretical conclusion reached in Chapter \ref{chap:litreview}).
\end{enumerate}

To proceed, these two aspects of design and implementation validity need to be converted in testable `hypotheses,' defined over measurable quantities, and amenable to some kind of empirical validation. This is much easier for correctness relative to superiority. In both cases, some assumptions are required.

\begin{enumerate}
    \item \textbf{Correctness:} testing correctness is relatively straight-forward. To test the correctness of targeted DGP sampling, I implicitly use the following testable hypothesis: Assuming the data metrics are a valid measure of distributional setting, changing the \textit{DGP Sampling Parameters} should produce observable changes in the data metrics for sampled DGP data, with changes in-line with the design-intent of each parameter. This hypothesis is manifestly testable, and the experiment to test it is performed in Chapter \ref{chap:macvalidation}. It is worth noting that confirming this hypothesis is not equivalent to confirming correctness. If the data metrics are invalid, then the targeting of the sampling is invalid. Further, there is no objective scale against which to evaluate the observed changes. So, even though changes are observed in the data metrics, each corresponding to a problem space axis, it is unclear if these changes span the range of the axis/cover the distributional problem space. Given the highly theoretical nature of the distributional problem space, I do not think that a more certain confirmation of targeted DGP sampling is possible.
    
    \item \textbf{Superiority:} testing superiority is a harder problem. As noted in Chapter \ref{chap:macvalidation}, producing different results for some inference method relative to another benchmarking approach is not indicative of either success/failure without a deeper understanding of why the results differ. The approach I take in Chapter \ref{chap:macvalidation} is to drastically simplify and reduce the surface of comparison. For example, I compare the performance results from a static DGP, taken from \textcite{Diamond2012GeneticStudies} and very similar sampled DGPs. This `similarity' is achieved by only sampling only the treatment mechanism while keeping the rest of the DGP the same and, further, sampling the treatment mechanism from the set of functions with identical form as the treatment mechanism in the static DGP. In this highly restricted context, it is possible to state a testable hypothesis that partially validates Maccabee's superiority: sampled DGPs will produce a distribution over estimator performance, with the static DGP appearing as a single sample from this distribution. Further, the sampled (and static) performance values should be correlated to the measurable distributional setting of the sampled DGP. If this hypothesis holds, it justifies both the general need for sampling - to capture enough of the distribution to provide accurate summary statistics - and the specific targeting of sampling based on distributional setting - given that distributional setting affects performance. Again, confirming this hypothesis does not amount to certain confirmation of superiority. It is possible that the results are an artifact of the chosen example or that they will not carry over to less restricted cases of sampling.
\end{enumerate}

Finally, I note that, even though the testable hypotheses above do not serve as absolute confirmation of validity, they do serve a purpose. The failure to \textbf{invalidate} the design and approach of Maccabee lends some credibility to its validity. Further tests of the same kind, which also failed to invalidate the approach, would lend further credence. This is how science works. We can never be 100\% sure of some ground-truth claim but can repeatedly fail to invalidate it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#dataviz}}
\label{hc:dataviz}

This paper is relatively light on examples of data visualization - much more effort was put into creating informative figures and tables to support theoretical analysis and explanations. I would, however, like to draw attention to the visualizations used in Chapter \ref{chap:macvalidation}, and specifically to figure \ref{fig:benchmark-validation-pure-synth-2}. 

\vspace{\baselineskip}

This figure communicates the key results relating to the importance of sampling DGPs to establish accurate measures of estimator performance. The challenge with constructing good data visualization, in this case, comes down to the complexity of the underlying data as well as of the concept being visualized. The core of the data is performance metric values for an estimator applied to data from a baseline, concrete DGP and a number of sampled, Maccabee DGPs. The goal of the figure is to demonstrate that a) there is substantial variance in estimator performance across sampled DGPs (but minimal variance within each DGP), b) that the performance of the estimator in sampled DGPs is correlated with the metric value that measures treatment mechanism linearity, and c) that the concrete DGP is just a single sample from the distribution over non-linearity and (correlated) performance. This information is displayed in the figure using a combination of a scatter plot, error bars, a regression line. Clear labeling, color, and captioning are used to make interpretation straightforward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Writing HC Applications}
\label{hc:section-writing}

\subsection*{\textbf{\#professionalism}}
\label{hc:professionalism}

\begin{itemize}
    \item This paper is written in \LaTeX \(  \) in order to produce a document with typesetting and quality of appearance appropriate for a thesis.
    
    \item The code that makes up the core contribution is open source to ensure that others can verify and extend it as they please. It is available \href{https://github.com/JoshBroomberg/Maccabee}{here}.
    
    \item The code is also made available as a pre-built python package hosted on the standard package repository, \href{https://pypi.org/project/maccabee/}{here}. This allows others to install it, and all required dependencies, with minimal effort. This is complemented by extensive documentation and tutorials to improve ease of use. Together, these two components embody the professional manner in which open source code should be distributed.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#organization}}
\label{hc:organization}

This work has extensive hierarchical structuring, designed to contribute to the ease of consumption and understanding. The paper is divided into parts which group the chapters related to foundational theory, literature review, and the design and implementation of the novel contribution (the Maccabee package). The chapters in each part cover logically distinct sets of material and are further divided into sections that contribute to the narrative of each chapter. Each chapter starts with an overview that explains this narrative, how it contributes to the overall work, and how it divided amongst the sections of the chapter.

\vspace{\baselineskip}

Beyond aiding in the linear consumption of the work, all of this structure, combined with the tools provided by \LaTeX, allows for the construction of a hyperlinked table of contents and extensive clickable cross-referencing throughout. These organizational features allow the reader to easily jump into and between sections in order to promote build an understanding of specific concepts or remind themselves of previous analysis when it is used later in the work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#audience}}
\label{hc:audience}

My goal was to make this work clear enough to be understood and appreciated by a reasonably technical reader without any specialized knowledge of causal inference. To this end, I wrote an Introduction (Chapter \ref{chap:intro}) and a theory review (Chapter \ref{chap:framework}) that are designed to connect the content of this paper to the broader (and more familiar) realm of general predictive modeling and to provide a solid conceptual foundation for the original analysis and synthesis in Chapters \ref{chap:problemspace} and \ref{chap:litreview}. The content in these chapters makes few assumptions about prior knowledge, thoroughly explaining all fundamental concepts (see \nameref{hc:context}). It also makes extensive use of common tools for technical communication - like the graphical models in Chapters \ref{chap:problemspace} and \ref{chap:macdesign} - to convey complex topics in a way that will be familiar to a technical audience (see \nameref{hc:medium}).

\vspace{\baselineskip}

The secondary goal of this paper, but the one which is of greatest practical importance, is to convince causal inference researchers that the approach proposed in this work is superior to existing approaches. And, to provide a path for them to adopt it in their own research. The applicable audience here is both more technical than the reader described above and requires a description of the approach/implementation at a much greater level of detail. Awareness of the needs of this audience inspired the creation of the online documentation (available \RTDlink{here}) - this documentation provides detailed descriptions of the implementation and how it can be used to benchmark methods of causal inference under varied active research conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#context}}
\label{hc:context}

As outlined in \nameref{hc:audience}, Chapter \ref{chap:intro} and \ref{chap:framework} are designed to provide the points of context that situate this work in the broader realm of general predictive modeling, and provide a solid conceptual foundation for the original analysis in later chapters. Writing this context requires striking a balance between going broad and basic enough to be useful but not so broad/basic that the context is uninteresting or unfocused. I attempted to achieve this by ensuring there is a coherent narrative that runs throughout the early chapters of the work, tying all the content together. I never introduce concepts in the form of arbitrary lists/sections and explain how each new idea connects to previous ideas and the larger narrative.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#medium}}
\label{hc:medium}

This HC is typically applied to analyzing the mediums used in creative work, but I think it has an important place in the construction of compelling scientific communication. The goal of scientific communication is less subjective than the goal of artistic communication, but the mediums used to construct the communication are equally as important. Two examples are worth mentioning:

\begin{enumerate}
    \item \textbf{Explaining code using rich, standalone documentation}: even-well written code is not optimized to convey all, and only, the information that a user needs to actually understand and use the code. This is especially true when you consider all of the boilerplate code that obfuscates the key information in any given file. Professional documentation tools, like the one I used, are designed to provide this information. This is a matter of a) for each component of the package, using page structure, spacing, and color to convey information about each object in a way that is easy to consume. Careful use of these visual components is required in order to combine code, written text, and results in a visually clear format. And b) using hyperlinks to allow for the navigation of a large network of interdependent components, each with their own detailed implementation documentation. Both the formatting and linking are made possible by the use of a web-based, programmatically-generated medium for the documentation.
    
    \item \textbf{Explaining models using graphical notation}: throughout the work (Chapters \ref{chap:framework}, \ref{chap:problemspace}, \ref{chap:macdesign}) I use graphical models to convey the relationship between random variables that define joint distributions/probabilistic models. This is a choice to use a specific visual medium to communicate information that could be communicated through math or text. This format is both a well-known standard and happens to be well-suited to the kind of generative models which are common in causal modeling. I believe that this format makes it much easier for readers to build an intuitive understanding of complex models and the underlying reality they describe.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Process HC Applications}
\label{hc:section-process}

\subsection*{\textbf{\#selfawareness}}
\label{hc:selfawareness}

The choice of this topic reflects an awareness of my strengths. I have a strong coding background, and so I knew from the outset that I would be able to produce the most valuable work if I chose a topic that was implementation-heavy. I also have a strong interest (and some skill) in statistical theory. This means that I enjoy work that builds from foundational (statistical) theory to reach novel/useful conclusions. I set out to find a topic that combined these two aspects. The later would ensure I was passionate about the work and willing to put in the effort to produce something of high quality. The former would ensure that I had the requisite skill to actually produce this high quality work.

\vspace{\baselineskip}

Initially, I settled on building a package to implement and benchmark methods of causal inference based on modern machine learning. This satisfied both of the aspects of my interest/strengths above. The goal was to make these advanced methods available to researchers without the skill required to implement machine learning-based methods. However, I ran into some practical issues in pursuing this idea. This required careful application of \nameref{hc:strategize}:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{\#strategize}}
\label{hc:strategize}

Early in the Capstone process, I realized, with the help of my advisor Professor Diamond, that the idea outlined above was a) too ambitious and would require an amount of work that was not possible within my time and resource constraints and b) that others, including a team at Microsoft, were working on similar packages. This meant there was little chance for me to contribute truly unique and valuable work.

\vspace{\baselineskip}

Fortunately, by this point, I had done enough initial research into the benchmarking component of the package to realize that there was interesting and valuable work to be done on approaches to benchmarking methods of causal inference. This aspect was not explored in the rapidly expanding literature on using machine learning for causal inference, so it represented a niche in which I could contribute. With this in mind, I pivoted to researching and building a package to support the benchmarking of all methods of causal inference. I was able to re-purposed my initial research on machine learning-based methods to justify why better methods of benchmarking would becoming increasingly important as inference methods became more complex and less amenable to formal/asymptotic performance analysis.

\vspace{\baselineskip}

This pivot was executed such that I ended up with a topic that was a) still in-line with my strengths and interests while b) being in a domain without aggressive competition, where I could contribute value as an undergrad and c) resulted in minimal wasted effort as I was able to re-purpose all of my existing research. I believe this demonstrates a strong example of \#strategic thought and action.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#modeling}}
% \label{hc:modeling} %todo remove?

% Applies extensively in constructing the statistical framework that is used to formalize the concept of a Data Generating Process. See Chapter \ref{chap:problemspace}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#estimation}}
% \label{hc:estimation} %todo remove?

% Applied to estimate the relative advantage of compilation vs. dynamic execution using the Big-O complexity approximation. Documented in Chapter \ref{chap:macimplementation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#biasmitigation}}
% \label{hc:biasmitigation} %todo remove?

% used in the review of existing methods when discussing how researchers may introduce bias by selecting functions that suit their own methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#communicationdesign}}
% \label{hc:communicationdesign} % todo remove

% used in the choice to make real online documentation that is easily consumed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#confidenceintervals}}
% \label{hc:confidenceintervals} %todo remove

% used in Chapter \ref{chap:macvalidation} when discussing benchmark validation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{\textbf{\#significance}}
% \label{hc:significance} %todo remove

% used in Chapter \ref{chap:macvalidation} when discussing benchmark validation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}