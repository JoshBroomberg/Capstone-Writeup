\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Causal Inference Theory}

\subsection*{\textbf{Capstone--\#CausalInferenceTheory}}
\label{lo:CausalInferenceTheory}

In Chapter \ref{chap:framework}, I outline the difference between predictive inference and causal inference, while recognizing the important overlap: The two types of inference are not truly separate, they are nested, with causal inference essentially being predictive inference under specific restrictions and assumptions. The analysis in the chapter utilizes and contrasts the two conventional frameworks for causal inference - Rubin's Potential Outcomes Framework and Pearl's Structural Causal Modeling. A key takeaway is that many different modeling approaches, and inference methods premised on these models, can be used to recover the same causal estimands. This is the premise that supports the importance of benchmarking - as a tool to determine which functionally equivalent inference approach is `best'.

\vspace{\baselineskip}

Chapter \ref{chap:problemspace} also makes use of basic causal inference theory in constructing the problem space. The construction of this space depends on understanding how the relationships between the various variables that make up a causal inference problem affect estimator performance. See \LOref{CausalProblemSpace} below for more detail.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CausalProblemSpace}}
\label{lo:CausalProblemSpace}

In Chapter \ref{chap:problemspace}, I describe the connection between the challenge of observational causal inference and the properties of the observed data. The analysis in this chapter establishes that the difficulty of accurate causal inference is highly dependent on the distributional setting of the data. Further, the chapter formalizes the description of the distributional setting using the language and tools of probability theory. IE, using random variables to describe each component of the data and then describing the distributional setting in terms of properties of the joint distribution that results from the specific (stochastic) functional relationships between the random variables. Different estimators may interact differently with different joint distributions - providing a technical foundation for the claim of differential performance first introduced in Chapter \ref{chap:framework} and mentioned in \LOref{CausalInferenceTheory} above.

\vspace{\baselineskip}

Chapters \ref{chap:litreview} and \ref{chap:macdesign} make use of the problem space introduced in Chapter \ref{chap:problemspace}. The analysis in these chapters demonstrates how the problem space can be used to a) understand the specific and general validity of existing benchmarking methods and b) design new methods with better validity properties given the way they handle benchmarking across the problem space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{CS146--\#GraphicalModels}}
\label{lo:GraphicalModels}

Graphical models are used throughout the paper to represent joint probability distributions in terms of the relationships between variables in the underlying (statistical) model. The way the notation is used varies chapter by chapter:

\begin{itemize}
    \item In Chapter \ref{chap:framework}, graphical models are used to demonstrate how causal inference is operationalized given observational data. Here, the primary purpose of the model is expositional, clarifying why and how the different pathways of dependency between variables must be untangled for accurate inference.
    
    \item In Chapter \ref{chap:litreview}, graphical models are used to represent the prototypal, generic Data Generating Process and the derived, concrete Data Generating Processes from the literature. Here, the (often implicit) graphical model representation of the data generation models aided in comparing different models.
    
    \item In Chapter \ref{chap:problemspace}, graphical models are used to aide the construction of the problem space that (hopefully) contains all performance-relevant variants of the joint distribution over the observed variables in a causal inference problem instance. Here, the graphical models are used to guide the permutation over the different, possible functional relations between the variables.
    
    \item In Chapter \ref{chap:macdesign}, graphical models are used to formalize the design of the stochastic benchmarking process. Here, the graphical model is both a communication tool - communicating a complex stochastic process to the reader - and a useful aide in the later implementation - helping to identify different sources of variance in the benchmark to guide aggregation and metric calculation.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{CS146--\#ProbabilityTheory}}
\label{lo:ProbabilityTheory}

The rubric for this LO describes the core skill as follows: "when provided with statements of probability, derive related statements of probability – involving marginal, conditional or joint distributions – using the sum rule, product rule, and Bayes' rule." The ability to perform these operations is demonstrated most clearly in the following places:

\begin{itemize}
    \item In Chapter \ref{chap:framework} (Section \ref{sec:challenge-of-causal-inference}), I implicitly manipulate conditional distributions using known independence relations to express the treatment effect as the true treatment effect plus selection bias. I then demonstrate how the same relation can be seen directly from a graphical model (which encodes the same independence information).
    
    \item In Chapter \ref{chap:problemspace} (Section \ref{sec:distro-setting-as-joint}), I use the product rule to construct the joint distribution over the observed data as the product of the marginal/conditional distributions that describe the observed and unobserved variables that make up a problem instance.
\end{itemize}

Beyond these two examples, probability theory is used extensively throughout the rest of the material in Chapters \ref{chap:framework} and \ref{chap:problemspace}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarking Theory and Design}

\subsection*{\textbf{Capstone--\#CausalBenchmarkTheory}}
\label{lo:CausalBenchmarkTheory}

Chapter \ref{chap:litreview} introduces and defines the concept of estimator quality as measured by one or more metric defined over the estimates produced by an estimator (values from the estimator's sampling distribution). It describes various strategies to calculate these metrics - analytical or asymptotic methods derive true expected values using the explicit analytical form of the estimator and input data distribution. This is only viable for data and estimators that are relatively simple. Monte Carlo methods calculate quality metrics by sampling from the estimator's sampling distribution and calculating approximate metric expectations. This is a specific application of the general Monte Carlo simulation approach to computing approximate distributional statistics. Following this foundational work, the chapter provides an in-depth discussion of the relative pros and cons of these two approaches to benchmarking causal inference algorithms. After reaching the conclusion that Monte Carlo approaches are superior, it then compares the validity of various Monte Carlo implementations/strategies from the literature. The specific and general validity criteria developed to support this discussion demonstrate a strong understanding of the strengths and weaknesses of Monte Carlo.

\vspace{\baselineskip}

Chapter \ref{chap:macdesign} discusses the design of the Monte Carlo benchmarking approach that is the core of this paper. The design is discussed in more detail under \LOref{CausalBenchmarkDesign} below. Here, I discuss the key theoretical insight that underlies this design: The idea that the single DGP used in standard Monte Carlo benchmarks is actually a `variable' that can affect the measured estimator performance. This means that all of the Monte Carlo designs discussed in Chapter \ref{chap:litreview}, should be thought of as drawing a single, potentially-biased sample from the distribution over problem instances. For robust and unbiased results, it is therefore important to sample multiple problem instances either from the whole space (support) of problem instances or from specific regions of interest. The content of this chapter effectively generalizes the core concept of Monte Carlo benchmarking to include the design space of the data generating process. This generalization requires a strong abstract understanding of how Monte Carlo methods operate in order to produce valid results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CausalBenchmarkDesign}}
\label{lo:CausalBenchmarkDesign}

\LOref{CausalBenchmarkTheory}, above, focuses on the theory behind methods for benchmarking causal inference algorithms. Here, the focus is on the concrete designs behind these methods.

\vspace{\baselineskip}

The design of a new Monte Carlo-based benchmarking method is the core contribution of this paper. This design is introduced in Chapter \ref{chap:macdesign}. The most novel aspect of this design is the mechanism (and associated parameterization) used to sample (the components of) the data generating process. This requires the ability to sample \textit{functions}, defined over externally-supplied (empirical) covariates, with functional forms that place the resultant DGP in the desired/specified region of the distributional problem space. The broad outline of the mechanism was taken from work by \textcite{Dorie2019Automated1}, who propose sampling from the class of generalized additive functions. I then extended this to allow for arbitrary empirical data and dynamic control over the desired region of the distributional problem space. Beyond this novel contribution, Chapter \ref{chap:macdesign} reflects the investment of substantial effort into producing a formal description of the Hybrid Monte Carlo benchmarking approach, which I expect to be useful in persuading other researchers of its merits.

\vspace{\baselineskip}

Chapter \ref{chap:macimplementation} introduces some of the important low-level components that are required to fully specify the design:

\begin{itemize}

    \item The \href{\RTDurl/design.html}{design page} describes the complete parameterization of the sampling procedure.
    
    \item The \href{\RTDurl/reference/modeling/performance-metrics.html}{performance metrics page} introduces the metrics that measure different aspects of estimator quality. These metrics are defined over the sampling distribution of estimand values produced by an estimator. Different metrics are applied for different causal estimands.
    
\end{itemize}

Finally, Chapter \ref{chap:macvalidation} performs experimentation to validate that the approach proposed in this paper works as intended and is superior to other methods (in the ways expected). Collecting and analyzing the evidence to support these conclusions requires a strong understanding of the differences between the compared Monte Carlo approaches and how these differences are expected to manifest in the benchmark output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarking Implementation}

\subsection*{\textbf{Capstone--\#CodeDesign}}
\label{lo:CodeDesign}

The design of the Maccabee package, outlined in Chapter \ref{chap:macimplementation}, demonstrates the ability to design a large technical project based on appropriate technical design principles. Following the rubric for this LO, I detail the use of four design principles below. It is important to note that Maccabee is designed in the object-oriented programming paradigm. This means the basic \textit{system components} are Python objects that consist of a collection of attributes (data) and methods that work together to \textit{encapsulate} logical state. This design paradigm informs the style in which the separation of concerns and abstraction principles manifest themselves.

\begin{enumerate}
    \item \textit{\textbf{Algorithmization}: demonstrate the ability to formulate algorithmic solutions to computational problems by systematically breaking problems down into a clear, ordered set of concrete steps.}
    
    I refer the grader to the \nameref{hc:algorithms} HC tag for a description of how algorithms were applied in the package design.
    
    \item \textit{\textbf{Separation of Concerns}: Design systems such that logically-separable tasks are handled by different system components, and each system component handles conceptually similar tasks.}
    
    Maccabee uses objects and modules to divide logically-separable functions and state. The \href{\RTDurl/reference.html}{package reference index page} provides an overview of the complete module and object design. The \textbf{\lstinline{maccabee.data_generation}} demonstrates the general pattern: The module contains a series of functionally-related classes, each of which performs a specific, and well-defined task:
    
    
    \begin{displayquote}
    The \textbf{\texttt{Data Generating Process Sampler}} object encapsulates the DGP sampling process and produces \textbf{\texttt{Data Generating Process}} instances. In turn, the \textbf{\texttt{Data Generating Process}} class encapsulate the logic required to sample data and, after instantiation by the sampling class, produces instances of the \textbf{\texttt{Generated Data Set}} class that contain the sampled data (both observed and unobserved) which is used for benchmarking. The section of the \href{\RTDurl/design.html}{design page} entitled \textit{Core Sampling Execution Flow} describes the complete set of object relations, providing many more examples of this kind of separation.
    \end{displayquote}
    
    This separation of concerns enables the extensive customizability that is possible when using the Maccabee package. A user can supply a custom version of each of the classes in the example above, possibly inheriting and maintaining most of the default behavior, in order to change aspects of the execution flow. This customization is explained in detail in the package documentation.
    
    It is worth mentioning that there are a number of instances in which objects are not used to achieve separation of concerns. This is done when using objects would unnecessarily complicate the user interface (sacrificing the quality of user-interface abstraction, as below). For example, the core benchmarking functions are module-level functions so that they can be used directly without the boilerplate required to instantiate an object instance. This is a stylistic choice made to simplify the interface and is acceptable under some use-case assumptions. In these cases, separation is still achieved at the module level.
    
    It is my opinion that a design that represents a good separation of concerns should be effectively self-documenting. IE, the user should have a clear idea of what each class does and how the classes fit together just from the class (and module) names. I believe that I have achieved this with Maccabee's design.
    
    \item \textit{\textbf{Abstraction}: Design interfaces that are easy to use and hide much of the complexity needed for the implementation.}
    
    Abstraction is closely related to the separation of concerns, as discussed above, but is focused on the design of the interface between the objects. The same example module as used above demonstrates how the interfaces in Maccabee are designed to hide the complexity involved in the execution flow:
    
    \begin{displayquote}
    After instantiating a \textbf{\texttt{Data Generating Process Sampler}}, the method \textbf{\lstinline{sample_dgp}} is used to sample a DGP. This method hides the complex set of internal processes that must be executed to sample the DGP. Similarly, the function \textbf{\lstinline{generate_dataset}} is used to sample a new data set from a \textbf{\texttt{Data Generating Process}} instance. These are both examples of a clean \textit{functional} interfaces enabling complex action. The \textbf{\texttt{Generated Data Set}} object provides an example of a clean \textit{data} interface. It abstracts away the complexity of the internal data structures used to store the sampled data and provides simple properties to access each of the observed and unobserved \textit{DGP variables} (as defined in Chapter \ref{chap:litreview}).
    \end{displayquote}
    
    The various pages of the \href{\RTDurl/reference.html}{package reference} document the interface of all of the objects which make up the Maccabee package. They are all designed with equivalent simplicity.
    
    One key advantage of this abstraction is the ability to implement advanced caching schemes, of the kind described in \nameref{hc:algorithms}, without increasing the complexity of downstream code. For example, there is an extensive, customizable caching scheme used in the \textbf{\texttt{Data Generating Process}} object. But this caching scheme is entirely encapsulated and is used automatically when the \textbf{\lstinline{generate_dataset}} function is called.
    
    \item \textit{\textbf{Complexity}: Analyze the asymptotic behavior of an algorithm solution to a problem and select solutions with this complexity in mind.}
    
    Attention was paid to complexity throughout the design process. I would like to draw attention to the complexity analysis documented \href{\RTDurl/advanced/parallelization.html}{here}. This analysis relates to the advantage of using autogenerated code compilation. Execution of raw, uncompiled sampled functions has a polynomial complexity in the number of covariates and a relatively large constant execution time per execution. Execution of compiled, sampled functions is constant in the number of covariates and has a relatively short constant execution time but comes with the penalty incurred from once-off compilation time (that is polynomial in the number of covariates). These dynamics mean that there is a covariate-count-dependent number of data samples at which point compilation is justified. Finding this point, and unlocking the optimization it enables, was only possible as a result of experimental complexity analysis. The code used to run this experimentation is available under the Process Notebooks section of the project repository (\href{https://github.com/JoshBroomberg/Maccabee/blob/master/Notebooks/Process\%20Notebooks/Code\%20Compilation\%20Validation.ipynb}{here}).
    
    Further considerations related to complexity are contained in the \nameref{hc:algorithms} HC tag. 
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CodeImplementation}}
\label{lo:CodeImplementation}

The actual code implementation should largely speak for itself. Below, I highlight some features that correspond to the points mentioned in the rubric:

\begin{itemize}
    \item Beyond the extensive doc-string documentation, I have included extensive inline comments to explain the vast majority of the implementation.
    
    \item Exceptions are thrown using custom Python exception types that provide useful guidance on the source and type of error.
    
    \item Logging is used to provide optional logs that document the execution flow for inspection.  This logging is implemented based on Python best practices and design patterns. Appropriate log levels are used for different messages. Messages of warning level or higher are output by default, using the \texttt{stderr} stream but detailed configuration is possible in user code. Loggers are configured using the package module hierarchy to allow for specific module-level control.
    
    \item The motivation behind the choice of performance-relevant data structures is explained in inline comments.
    
    \item Advanced python features are used to produce complex functionality without complex code. For example, a python Metaclass is used to implement the DSL that enables easy DGP specification. This provides access to the DGP class properties at class instantiation time, allowing validation and preprocessing of the DGP execution flow once per class definition.
    
    \item As explained under \nameref{hc:algorithms} and \LOref{CodeDesign} extensive experimentation and design iteration was performed to reduce code execution complexity and improve speed (within the same formal complexity class). I estimate that my final version runs a typical-sized benchmark in about 10\% of the time taken by the first version. Most of the speed-up was achieved by optimizations like caching, compilation, and parallelization, which improved execution time without changing the actual big-O complexity of the benchmarking `algorithm.'
\end{itemize}

Beyond this aspects of the implementation, I would like to draw attention to the implementation of the code used to run the validation that appears in Chapter \ref{chap:macvalidation}. In implementing this code, I acted as a user of the Maccabee - utilizing the extensibility designed into the package to implement an advanced use case. I was able to implement the custom sampled data generating process and a static data generating process needed for the experiment with no changes to the core operational code of the package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CodeUsability}}
\label{lo:CodeUsability}

Two important steps are taken to ensure code usability:

\begin{enumerate}
    \item I authored extensive documentation and tutorials to enable users to understand and extract value from code without reading the code itself. This documentation was built using Sphinx and ReadTheDocs. This has two advantages. For the user, it means easy access to hosted docs with all of the tools required for easy consumption: search, hyper-linking, direct access to the code documented on each page, etc. For the developer, it means that documentation is stored alongside the code and can be easily modified as the code is further developed. The docs are rebuilt by ReadTheDocs on each git commit.
    
    \item I deployed the code as a Python package that can be installed with industry-standard tools - (\texttt{pip} and \texttt{setuptools}). The code is updated by an automated continuous deployment process that deploys tagged git commits. This ensures the latest (stable) code is available to users. I also built a docker image that includes all of the dependencies and allows users to quickly access the package for initial exploration. This is an emerging best-practice.
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}