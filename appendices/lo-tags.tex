\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Causal Inference Theory}

\subsection*{\textbf{Capstone--\#CausalInferenceTheory}}
\label{lo:CausalInferenceTheory}

In Chapter \ref{chap:framework}, I outline the difference between predictive inference and causal inference, while recognizing the important overlap: The two types of inference are not truly separate, they are nested, with causal inference essentially being predictive inference under specific restrictions and assumptions. The analysis in the chapter utilizes and contrasts the two conventional frameworks for causal inference - Rubin's Potential Outcomes Framework and Pearl's Structural Causal Modeling. A key takeaway is that many different modeling approaches, and inference methods premised on these models, can be used to recover the same causal estimands. This is the premise that supports the importance of benchmarking - as a tool to determine which functionally equivalent inference approach is `best'.

\vspace{\baselineskip}

Chapter \ref{chap:problemspace} also makes use of basic causal inference theory in constructing the problem space. The construction of this space depends on understanding how the relationships between the various variables that make up a causal inference problem affect estimator performance. See \LOref{CausalProblemSpace} below for more detail.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CausalProblemSpace}}
\label{lo:CausalProblemSpace}

In Chapter \ref{chap:problemspace}, I describe the connection between the challenge of observational causal inference and the properties of the observed data. The analysis in this chapter establishes that the difficulty of accurate causal inference is highly dependent on the distributional setting of the data. Further, the chapter formalizes the description of the distributional setting using the language and tools of probability theory. IE, using random variables to describe each component of the data and then describing the distributional setting in terms of properties of the joint distribution that results from the specific (stochastic) functional relationships between the random variables. Different estimators may interact differently with different joint distributions - providing a technical foundation for the claim of differential performance first introduced in Chapter \ref{chap:framework} and mentioned in \LOref{CausalInferenceTheory} above.

\vspace{\baselineskip}

Chapters \ref{chap:litreview} and \ref{chap:macdesign} make use of the problem space introduced in Chapter \ref{chap:problemspace}. The analysis in these chapters demonstrates how the problem space can be used to a) understand the specific and general validity of existing benchmarking methods and b) design new methods with better validity properties given the way they handle benchmarking across the problem space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{CS146--\#GraphicalModels}}
\label{lo:GraphicalModels}

Graphical models are used throughout the paper to represent joint probability distributions in terms of the relationships between variables in the underlying (statistical) model. The way the notation is used varies chapter by chapter:

\begin{itemize}
    \item In Chapter \ref{chap:framework}, graphical models are used to demonstrate how causal inference is operationalized given observational data. Here, the primary purpose of the model is expositional, clarifying why and how the different pathways of dependency between variables must be untangled for accurate inference.
    
    \item In Chapter \ref{chap:litreview}, graphical models are used to represent the prototypal, generic Data Generating Process and the derived, concrete Data Generating Processes from the literature. Here, the (often implicit) graphical model representation of the data generation models aided in comparing different models.
    
    \item In Chapter \ref{chap:problemspace}, graphical models are used to aide the construction of the problem space that (hopefully) contains all performance-relevant variants of the joint distribution over the observed variables in a causal inference problem instance. Here, the graphical models are used to guide the permutation over the different, possible functional relations between the variables.
    
    \item In Chapter \ref{chap:macdesign}, graphical models are used to formalize the design of the stochastic benchmarking process. Here, the graphical model is both a communication tool - communicating a complex stochastic process to the reader - and a useful aide in the later implementation - helping to identify different sources of variance in the benchmark to guide aggregation and metric calculation.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{CS146--\#ProbabilityTheory}}
\label{lo:ProbabilityTheory}

The rubric for this LO describes the core skill as follows: "when provided with statements of probability, derive related statements of probability – involving marginal, conditional or joint distributions – using the sum rule, product rule, and Bayes' rule." The ability to perform these operations is demonstrated most clearly in the following places:

\begin{itemize}
    \item In Chapter \ref{chap:framework} (Section \ref{sec:challenge-of-causal-inference}), I implicitly manipulate conditional distributions using known independence relations to express the treatment effect as the true treatment effect plus selection bias. I then demonstrate how the same relation can be seen directly from a graphical model (which encodes the same independence information).
    
    \item In Chapter \ref{chap:problemspace} (Section \ref{sec:distro-setting-as-joint}), I use the product rule to construct the joint distribution over the observed data as the product of the marginal/conditional distributions that describe the observed and unobserved variables that make up a problem instance.
\end{itemize}

Beyond these two examples, probability theory is used extensively throughout the rest of the material in Chapters \ref{chap:framework} and \ref{chap:problemspace}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarking Theory and Design}

\subsection*{\textbf{Capstone--\#CausalBenchmarkTheory}}
\label{lo:CausalBenchmarkTheory}

Chapter \ref{chap:litreview} introduces and defines the concept of estimator quality as measured by one or more metric defined over the estimates produced by an estimator (values from the estimator's sampling distribution). It describes various strategies to calculate these metrics - analytical or asymptotic methods derive true expected values using the explicit analytical form of the estimator and input data distribution. This is only viable for data and estimators that are relatively simple. Monte Carlo methods calculate quality metrics by sampling from the estimator's sampling distribution and calculating approximate metric expectations. This is a specific application of the general Monte Carlo simulation approach to computing approximate distributional statistics. Following this foundational work, the chapter provides an in-depth discussion of the relative pros and cons of these two approaches to benchmarking causal inference algorithms. After reaching the conclusion that Monte Carlo approaches are superior, it then compares the validity of various Monte Carlo implementations/strategies from the literature. The specific and general validity criteria developed to support this discussion demonstrate a strong understanding of the strengths and weaknesses of Monte Carlo.

\vspace{\baselineskip}

Chapter \ref{chap:macdesign} discusses the design of the Monte Carlo benchmarking approach that is the core of this paper. The design is discussed in more detail under \LOref{CausalBenchmarkDesign} below. Here, I discuss the key theoretical insight that underlies this design: The idea that the single DGP used in standard Monte Carlo benchmarks is actually a `variable' that can affect the measured estimator performance. This means that all of the Monte Carlo designs discussed in Chapter \ref{chap:litreview}, should be thought of as drawing a single, potentially-biased sample from the distribution over problem instances. For robust and unbiased results, it is therefore important to sample multiple problem instances either from the whole space (support) of problem instances or from specific regions of interest. The content of this chapter effectively generalizes the core concept of Monte Carlo benchmarking to include the design space of the data generating process. This generalization requires a strong abstract understanding of how Monte Carlo methods operate in order to produce valid results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CausalBenchmarkDesign}}
\label{lo:CausalBenchmarkDesign}

\LOref{CausalBenchmarkTheory}, above, focuses on the theory behind methods for benchmarking causal inference algorithms. Here, the focus is on the concrete designs behind these methods.

\vspace{\baselineskip}

The design of a new Monte Carlo-based benchmarking method is the core contribution of this paper. This design is introduced in Chapter \ref{chap:macdesign}. The most novel aspect of this design is the mechanism (and associated parameterization) used to sample (the components of) the data generating process. This requires the ability to sample \textit{functions}, defined over externally-supplied (empirical) covariates, with functional forms that place the resultant DGP in the desired/specified region of the distributional problem space. The broad outline of the mechanism was taken from work by \textcite{Dorie2019Automated1}, who propose sampling from the class of generalized additive functions. I then extended this to allow for arbitrary empirical data and dynamic control over the desired region of the distributional problem space. Beyond this novel contribution, Chapter \ref{chap:macdesign} reflects the investment of substantial effort into producing a formal description of the Hybrid Monte Carlo benchmarking approach, which I expect to be useful in persuading other researchers of its merits.

\vspace{\baselineskip}

Chapter \ref{chap:macimplementation} introduces some of the important low-level components that are required to fully specify the design:

\begin{itemize}

    \item The \href{\RTDurl/design.html}{design page} describes the complete parameterization of the sampling procedure.
    
    \item The \href{\RTDurl/reference/modeling/performance-metrics.html}{performance metrics page} introduces the metrics that measure different aspects of estimator quality. These metrics are defined over the sampling distribution of estimand values produced by an estimator. Different metrics are applied for different causal estimands.
    
\end{itemize}

Finally, Chapter \ref{chap:macvalidation} performs experimentation to validate that the approach proposed in this paper works as intended and is superior to other methods (in the ways expected). Collecting and analyzing the evidence to support these conclusions requires a strong understanding of the differences between the compared Monte Carlo approaches and how these differences are expected to manifest in the benchmark output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarking Implementation}

\subsection*{\textbf{Capstone--\#CodeDesign}}
\label{lo:CodeDesign}

The design of the Maccabee package, outlined in Chapter \ref{chap:macimplementation}, demonstrates the ability to design a large technical project based on appropriate technical design principles. Following the rubric for this LO, I detail the use of four design principles below\footnote{These principles align with the core LOs of the CS162 Software Development class.}. It is important to note that Maccabee is designed in the object-oriented programming paradigm. This means the basic \textit{system components} are Python objects that consist of a collection of attributes (data) and methods that work together to \textit{encapsulate} logical state. This design paradigm informs the style in which the separation of concerns and abstraction principles manifest themselves.

\begin{enumerate}
    \item \textit{\textbf{Algorithmization}: demonstrate the ability to formulate algorithmic solutions to computational problems by systematically breaking problems down into a clear, ordered set of concrete steps.}
    
    I refer the grader to the \nameref{hc:algorithms} HC tag for a description of how algorithms were applied in the package design.
    
    \item \textit{\textbf{Separation of Concerns}: Design systems such that logically-separable tasks are handled by different system components, and each system component handles conceptually similar tasks.}
    
    Maccabee uses objects and modules to divide logically-separable functions and state. The \href{\RTDurl/reference.html}{package reference index page} provides an overview of the complete module and object design. The \textbf{\lstinline{maccabee.data_generation}} demonstrates the general pattern: The module contains a series of functionally-related classes, each of which performs a specific, and well-defined task:
    
    
    \begin{displayquote}
    The \textbf{\texttt{Data Generating Process Sampler}} object encapsulates the DGP sampling process and produces \textbf{\texttt{Data Generating Process}} instances. In turn, the \textbf{\texttt{Data Generating Process}} class encapsulate the logic required to sample data and, after instantiation by the sampling class, produces instances of the \textbf{\texttt{Generated Data Set}} class that contain the sampled data (both observed and unobserved) which is used for benchmarking. The section of the \href{\RTDurl/design.html}{design page} entitled \textit{Core Sampling Execution Flow} describes the complete set of object relations, providing many more examples of this kind of separation.
    \end{displayquote}
    
    This separation of concerns enables the extensive customizability that is possible when using the Maccabee package. A user can supply a custom version of each of the classes in the example above, possibly inheriting and maintaining most of the default behavior, in order to change aspects of the execution flow. This customization is explained in detail in the package documentation.
    
    It is worth mentioning that there are a number of instances in which objects are not used to achieve separation of concerns. This is done when using objects would unnecessarily complicate the user interface (sacrificing the quality of user-interface abstraction, as below). For example, the core benchmarking functions are module-level functions so that they can be used directly without the boilerplate required to instantiate an object instance. This is a stylistic choice made to simplify the interface and is acceptable under some use-case assumptions. In these cases, separation is still achieved at the module level.
    
    It is my opinion that a design that represents a good separation of concerns should be effectively self-documenting. IE, the user should have a clear idea of what each class does and how the classes fit together just from the class (and module) names. I believe that I have achieved this with Maccabee's design.
    
    \item \textit{\textbf{Abstraction}: Design interfaces that are easy to use and hide much of the complexity needed for the implementation.}
    
    Abstraction is closely related to the separation of concerns, as discussed above, but is focused on the design of the interface between the objects. The same example module as used above demonstrates how the interfaces in Maccabee are designed to hide the complexity involved in the execution flow:
    
    \begin{displayquote}
    After instantiating a \textbf{\texttt{Data Generating Process Sampler}}, the method \textbf{\lstinline{sample_dgp}} is used to sample a DGP. This method hides the complex set of internal processes that must be executed to sample the DGP. Similarly, the function \textbf{\lstinline{generate_dataset}} is used to sample a new data set from a \textbf{\texttt{Data Generating Process}} instance. These are both examples of a clean \textit{functional} interfaces enabling complex action. The \textbf{\texttt{Generated Data Set}} object provides an example of a clean \textit{data} interface. It abstracts away the complexity of the internal data structures used to store the sampled data and provides simple properties to access each of the observed and unobserved \textit{DGP variables} (as defined in Chapter \ref{chap:litreview}).
    \end{displayquote}
    
    The various pages of the \href{\RTDurl/reference.html}{package reference} document the interface of all of the objects which make up the Maccabee package. They are all designed with equivalent simplicity.
    
    One key advantage of this abstraction is the ability to implement advanced caching schemes, of the kind described in \nameref{hc:algorithms}, without increasing the complexity of downstream code. For example, there is an extensive, customizable caching scheme used in the \textbf{\texttt{Data Generating Process}} object. But this caching scheme is entirely encapsulated and is used automatically when the \textbf{\lstinline{generate_dataset}} function is called.
    
    \item \textit{\textbf{Complexity}: Analyze the asymptotic behavior of an algorithm solution to a problem and select solutions with this complexity in mind.}
    
    Attention was paid to complexity throughout the design process. I would like to draw attention to the complexity and scaling analysis presented \href{\RTDurl/advanced/parallelization.html}{here}. This analysis establishes when and why the compilation of sampled DGP functions yields an execution speed up. This speedup is dynamic and only exists in certain regions of the benchmark configuration/parameter space. Characterizing these regions, and unlocking the optimization this characterization enabled, was only possible as a result of experimental analysis combined with theoretical knowledge of scaling and complexity relations.
    
    Further, smaller considerations related to complexity were outlined in the \nameref{hc:algorithms} HC tag. 
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CodeImplementation}}
\label{lo:CodeImplementation}

The actual code implementation should largely speak for itself. Below, I highlight some features that correspond to the points mentioned in the rubric:

\begin{itemize}
    \item I have included extensive inline comments to explain the implementation to improve readability and extensibility of the of the code. This documentation is in the form of doc-strings that are parsed to produce the online documentation as well as granular line-level comments for all complex operations.
    
    \item Exceptions are thrown using either appropriate built-in exception types or custom Python exceptions. This ensures that the exceptions provide useful guidance on the source and type of all critical errors.
    
    \item Logging is used to provide optional logs that document the execution flow for inspection.  This logging is implemented based on Python best practices and design patterns. Appropriate log levels are used for different messages. Messages of warning level or higher are output by default, using the \texttt{stderr} stream but detailed configuration is possible in user code. Loggers are configured using the package module hierarchy to allow for specific module-level control. See the logging tutorial included in the online documentation for more detail.
    
    \item The motivation behind the choice of performance-relevant data structures is explained in inline comments. I would like to note to specific, important data structure choices here:
    
    \begin{itemize}
        \item \texttt{Pandas.DataFrames} are used throughout the package for storing, passing and accessing empirical and generated data sets. This data structure allows for convenient indexing of the data on column name and filtering/grouping in numerous ways. Both types of access are used throughout the data generation process.
        
        \item \textit{Abstract Syntax Trees (ASTs)} for expression storage and manipulation. ASTs refer to (standard) trees that are used to represent complex structures made up of arbitrary data and operations that are applied to/produce the data. For example, the code that makes up any expression in Python can be represented in the form of an AST with nodes representing variables, constants, functions and operators. In Maccabee, the \texttt{Sympy} library is used to build and modify ASTs that represent mathematical expressions defined over symbols and operators that operate on one or more symbol (addition, exponentiation etc). Each sampled covariate transform is an AST - with between one and three covariates and one or more mathematical operators. The individual transform ASTs are merged into larger trees representing the sampled functions. Normalization is then performed through the use of additional nodes that modify the output of the merged tree. As is evident from this description, ASTs are useful because they allow for the programmatic construction and manipulation of mathematical expressions. These expressions can then converted into code that represents the mathematical functions.
    \end{itemize}
    
    \item Advanced python features are used to produce complex functionality without complex code. For example, a python Metaclass is used to implement the DSL that enables easy DGP specification. This provides access to the DGP class properties at class instantiation time, allowing validation and preprocessing of the DGP execution flow once per class definition.
    
    \item As explained under \nameref{hc:algorithms} and \LOref{CodeDesign} extensive experimentation and design iteration was performed to reduce code execution complexity and improve speed (within the same formal complexity class). I estimate that my final version runs a typical-sized benchmark in about 10\% of the time taken by the first version. Most of the speed-up was achieved by optimizations like caching, compilation, and parallelization, which improved execution time without changing the actual big-O complexity of the benchmarking `algorithm.'
\end{itemize}

Beyond this aspects of the implementation, I would like to draw attention to the implementation of the code used to run the validation that appears in Chapter \ref{chap:macvalidation}. In implementing this code, I acted as a user of the Maccabee - utilizing the extensibility designed into the package to implement an advanced use case. I was able to implement the custom sampled data generating process and a static data generating process needed for the experiment with no changes to the core operational code of the package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\textbf{Capstone--\#CodeUsability}}
\label{lo:CodeUsability}

Two important steps are taken to ensure code usability:

\begin{enumerate}
    \item I authored extensive documentation and tutorials to enable users to understand and extract value from code without reading the code itself. This documentation was built using Sphinx and ReadTheDocs. This has two advantages. For the user, it means easy access to hosted docs with all of the tools required for easy consumption: search, hyper-linking, direct access to the code documented on each page, etc. For the developer, it means that documentation is stored alongside the code and can be easily modified as the code is further developed. The docs are rebuilt by ReadTheDocs on each git commit.
    
    \item I deployed the code as a Python package that can be installed with industry-standard tools - (\texttt{pip} and \texttt{setuptools}). The code is updated by an automated continuous deployment process that deploys tagged git commits. This ensures the latest (stable) code is available to users. I also built a docker image that includes all of the dependencies and allows users to quickly access the package for initial exploration. This is an emerging best-practice.
    
\end{enumerate}

\section{CP194}

Much of what I have to say regarding the CP194 LOs has been covered in the other HC and LO tags. The tags below make a handful of small additions, but I largely defer to the content in the tags above.

\subsection*{\textbf{CP194--\#qualitydeliverables}}

My goal for this project was to deliver work that I was proud of and that contributed some small amount of value to the field of causal inference. Speaking to the first point, I am proud to present theoretical work that is deeply sourced and carefully thought through and technical work that is based on countless hours of implementation and refactoring. It also seems, based on initial feedback from Prof Diamond, that this work could be of value to the research community.

\vspace{\baselineskip}

I am also particularly proud to be delivering a project that involves a breadth and depth of content/thought that is too large to fit into my head at one time. This is the first time that I have had the experience of working on something of this scale. Producing high-quality work in this paradigm required learning how to use tools and processes that enabled me to return to old thoughts and pick them up later to continue work. For example, I kept an up-to-date high-level overview of the whole project so that I could see the evolving connections between sections and jump back into editing quickly. I hope that these tools and processes allowed me to produce a single, cohesive, high-quality work product.

\subsection*{\textbf{CP194--\#accountability}}

I hold myself to a high standard, and I think that has been evident in the deliverables presented throughout this year. My advisor had to do little-to-no prodding to get me to work harder/faster, and the work presented here represents writing and implementation executed throughout the last year (rather than rushed out over the last few weeks). I think this is 90\% of what is required to be properly \#accountable for Capstone. The \nameref{hc:responsibility} tag speaks, in more detail, to the ownership I have taken and my commitment to driving this project forward. The \nameref{hc:strategize} tag speaks to my resilience in the face of initial problems created by an overly ambitious and poorly scoped project. The path to this point was by no means straight, but I have worked hard throughout.

\subsection*{\textbf{CP194--\#connect}}

I was fortunate to have an advisor, Professor Diamond, who had the exact expertise and interests required to guide me in this project. This greatly reduced the necessity of finding external subject matter experts. I think it is important to note that this alignment (and productive relationship) with my advisor was not the product of chance. I initially reached out to Professor Diamond to discuss my idea for this project in the Spring semester of Junior year. The initial conversation helped shape my research direction prior to Senior year. Based on this, it was almost a happy co-incidence that I could ask for Prof Diamond to serve as my formal advisor in addition to serving the role of an external subject matter expert.

\subsection*{\textbf{CP194--\#feedback}}

Braden Scherting and I chose to provide each other feedback for both of the formal feedback assignments this year. This was a decision designed to reap the benefit of feedback from a peer who was increasingly familiar with my work. This strategy resulted in a few interesting and in-depth conversations with Braden that helped to focus my initial effort as I crafted new content. I would like to express particular appreciation for the guidance Braden provided on both the content and tone of the initial documentation draft as this resulted in substantial changes which improved the quality of this work prodct.

\vspace{\baselineskip}

I also pursued a similar ongoing feedback relationship with two technically-inclined friends outside of Minerva. After each hand-in of `complete' work (at the end of the Fall semester and in week 10 of the Spring semester), I reached out to these friends and asked for feedback on my draft. This feedback, from non-expert but technically-competent readers, helped me to fine tune the explanations in the early chapters of this work to the correct level of technicality and detail.

\subsection*{\textbf{CP194--\#research}}

The HC tags in Appendix \ref{hc:section-analytical} extensively document the steps taken to base this work on in-depth research into the relevant academic literature.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}