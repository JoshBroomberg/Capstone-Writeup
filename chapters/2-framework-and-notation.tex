\documentclass[../main.tex]{subfiles}
\begin{document}

This chapter provides a theoretical overview of observational causal inference. This serves as a foundation for the rest of the work, introducing the notation and framing which are used throughout. The framing of causal inference introduced in this chapter is primarily based on Rubin’s Potential Outcome Framework as outlined in \textcite{Holland1986StatisticsInference} with augmentation from Pearl’s Structural Causal Models as outlined in \textcite{Pearl2009CausalOverview}.\par

\vspace{\baselineskip}
Per Holland, observational causal inference applies statistical methods to measure the \textit{effects of causes} in non-experimental settings\textit{. }IE, methods of observational causal inference seek to estimate the quantitative effect of some \textit{treatment }on \textit{units} with the potential - realized or not - to be exposed to that treatment without the use of randomized exposure\footnote{ The term treatment is adopted from experimental literature for clarity. }. Note that if the unit has no potential to be exposed to the treatment or is always exposed, then understanding the effect of the treatment on that unit is philosophically impossible: the effect of a cause is defined based on the observed outcome relative to some (hypothetical) \textit{counterfactual }in which the cause did not occur. These ideas can be formalized by expressing them through a quantitative, statistical lens as follows.\par

\section{Notation and Estimands}

\vspace{\baselineskip}
Let there be a population of units -  \( U \) - with individual units from  \( U \)  indexed as  \( u_{i} \) . Let  \( Z \left( u_{i} \right)  = Z_{i} \) be an indicator variable that tracks whether a unit was exposed to the treatment \footnote{ This implies a binary treatment regime in which treatment is either present or absent for each unit, but the framework present in the text can be extended to multiple discrete or single continuous treatments. }. Based on the binary nature of the treatment, each unit has two \textit{potential outcomes }-  \( Y_{1} \left( u \right)  \) and  \( Y_{0} \left( u \right)  \)  - which measure the outcome the unit experiences under treatment or absence of treatment (referred to as the \textit{control }condition, following experimental nomenclature). Further, each unit has a (potentially vector-valued) covariate measurement  \( X_{i} \) . The name covariate is used to imply that the variable  \( X \) may co-vary (or be \textit{correlated}) with the outcomes,  \( Y_{1} \) and  \( Y_{0} \) , and the treatment status  \( Z \) across the population \( U \) . In the real world, a correlation between these variables may result from the covariates in  \( X \) causally affecting the outcome/treatment assignment or the inverse. For simplicity, I assume for the rest of this section that the covariates are measured pre-treatment and, thus, are not causally affected by a unit’s treatment assignment or outcome(s) \footnote{ Resolving the truth of this assumption is crucially important in practice - see \textcite{Pearl2009CausalOverview} for the risks of using covariates which are causally affected by treatment/outcome (even if the measurement is pre-treatment). However, this consideration is not important for establishing the fundamental operation of causal inference.}. There are two types of covariates depending on the underlying causal mechanism: \textit{confounders} are causally related to both the treatment and outcome of units in the population while \textit{sources of heterogeneity }are related to either, but not both, the treatment or outcome. The reason for this naming will become clear shortly.\par

\vspace{\baselineskip}
Given the above, the effect of a cause on an individual can be defined as  \(  \tau_{i}=Y_{1} \left( u_{i} \right)  - Y_{0} \left( u_{i} \right)   \) . \textit{The Fundamental Problem of Causal Inference,} per \textcite{Holland1986StatisticsInference}, is that $``$it is impossible to observe the value of  \( Y_{1} \left( u_{i} \right)  \)  and  \( Y_{0} \left( u_{i} \right)  \) for the same unit and, therefore, it is impossible to observe the effect of t on u$"$ . For any given unit, we only observe a single outcome defined by  \( Y \left( u \right)  =Z_{i} \times Y_{1} \left( u \right) + \left( 1-Z_{i} \right)  \times Y_{0} \left( u \right)  \) - the outcome under the treatment which the unit experienced. At this point, it may appear that the idea of causal inference is hopeless. However, introducing statistical tools (more specifically expectations), provides a path forward. The first step is to define average effect estimands in terms of observed and unobserved potential outcomes. These causal inference \textit{estimands} can then be targeted by causal inference \textit{estimators }defined over only observed data. Define the \textit{population} \textit{average treatment effect }(PATE) as:\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( PATE = E \left[  \tau \right]  = E \left[ Y_{1} - Y_{0} \right]  = E \left[ Y_{1} \right]  - E \left[ Y_{0} \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
This is the most basic causal estimand. It is possible to construct analogous expressions for samples rather than populations and for the treated/control subgroups of the population/sample. However, for the purposes of this paper, the only other important estimator is the \textit{conditional average treatment effect }(CATE) which refers to the causal effect conditioned on a covariate observation  \( X_{i} \) :\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( CATE= E \left[ Y_{1} - Y_{0}  \vert  X_{i} \right]  =E \left[ Y_{1}  \vert  X_{i} \right]  - E \left[  Y_{0}  \vert  X_{i} \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
Note that the PATE can be recovered by averaging the CATE over all of the units in the population based on the formula below. This allows us to focus our attention on this estimand.\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( PATE = \frac{1}{n} \sum _{i=1}^{n}E \left[ Y_{1} - Y_{0}  \vert  X_{i} \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
Given than two units with an identical observed  \( X_{i} \) are effectively indistinguishable in experimental terms, the CATE is often referred to as the \textit{individual average treatment effect \footnote{ The word average is used to indicate that units with identical X\_i values may, in fact, have different treatment effects in which case the CATE is an average over these effects for units indistinguishable on the observed covariates. }.}\par

\section{The Challenge of Accurate Causal Inference}

\vspace{\baselineskip}
The estimands above imply that if we can accurately approximate the expectation of the two potential outcomes across the population, or a covariate level, then we can approximate the average treatment effects. IE, it is possible to estimate the unobservable counterfactual at the individual level using expectations for the potential outcomes defined over the population of units. However, estimating these expectations is challenging: In any experimental or observational setting - rather than observing  \( E \left[ Y_{1} \right]  \)  and  \( E \left[ Y_{0} \right]  \) , we observe  \( E \left[ Y_{1}  \vert  Z=1 \right]   \) and  \( E \left[ Y_{0}  \vert  Z= 0 \right]  \) . This expresses the idea that we observe the outcome conditioned on treatment assignment. There is no guarantee that  \( E \left[ Y_{1} \right]  = E \left[ Y_{0}  \vert  Z=1 \right]  \)  and, in fact, this is highly unlikely in observation settings. This is where the notion of selection bias become important.\par


\vspace{\baselineskip}
Following Holland, let us call the average treatment effect arrived at from the observed outcomes the treatment effect prima facie:\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( T_{pf}=E \left[ Y_{1}  \vert  Z=1 \right]  - E \left[ Y_{0}  \vert  Z=0 \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
Observe that if the treatment assignment is independent of the potential outcomes -  \( P \left( Z  \vert  Y_{1},~Y_{0} \right)  = P \left( Z \right)  \)  - then  \( E \left[ Y_{1}  \vert  Z=1 \right]  \) does indeed equal  \( E \left[ Y_{1} \right]  \)  and the true average treatment effect is recovered by the treated/control group estimates. This explains the power of randomized control trials. If treatment/control is assignment randomly then the condition above holds and the group outcome averages allow recovery of the true effect. In the absence of randomization, it is possible for the treat/control group average outcomes to be ‘biased’ by the systematic inclusion of units which have a better/worse potential outcome under treatment. This bias can be neatly formalized. Assume a constant treatment effect  \( T \) . Then for the treated group we have:  \( E \left[ Y_{1}  \vert  Z=1 \right]  = E \left[ Y_{0}  \vert  Z=1 \right]  + T \) . Substituting this into the formula for  \( T_{pf} \) , we arrive at:\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( T_{pf} = T + \left( E \left[ Y_{0}  \vert  Z = 1 \right]  - E \left[ Y_{0}  \vert  Z =0 \right]  \right)  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
The prima facie effect is the true effect plus the expected difference in the outcome under control for the treated and control groups. In the absence of randomization, systemic differences between the treatment and control group induce bias. The challenge of observation causal inference is how to undo the bias induced by these differences. Before proceeding to strategies for achieving this, it is important to explore the mechanisms which create the systemic difference in the first place. To do this, I draw on Structural Causal Models (SCMs) introduced by Pearl and summarized in \textcite{Pearl2009CausalOverview}. SCMs are useful because they provide an intuitive way to communicate the causal mechanisms which give rise to the bias discussed above. This understanding, in turn, makes it easier to understand the construction of bias-free causal inference estimators.\par


\vspace{\baselineskip}
Pearl’s SCMs relate the variables defined above - outcome, treatment, and covariates - through a directed acyclic graph. The nodes in this graph are the variables, and edges between nodes indicate causal dependency between the child node variable (the target of the directed edge) and its parent(s) (the origin of the edge(s)). More precisely, the value of each variable node is defined in terms of a function that depends solely on the values of the parents of the node in the graph. This implies that knowing the value of all the parent nodes of a child node fully determines the value of the child node\footnote{ In a complete SCM, each node has a parent that presents a source of uncorrelated noise, which explains the absence of perfect predictive power of the observed parents. For simplicity, this noise node is omitted from the models presented in the text. This implies that all the variables are fully determined by their parents. }. As a result of this causal dependence, variables will be correlated, to some degree, with their parents in the graph, to the ancestors of those parents, and to any other nodes which share common parents/ancestors. With this definition established, I present the simple SCM in Figure \ref{fig:confounder-scm} to explain the origin of selection bias in causal effect estimation and clarify the difference between causal and predictive inference.

\vspace{\baselineskip}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\linewidth]{ch2-confounder-scm.png}
    \caption{SCM displaying causal relations between the outcome, treatment status and covariates which gives rise to selection bias.}
    \label{fig:confounder-scm}
\end{figure}

This SCM displays the simplest possible situation in which selection bias is a problem. The edge between  \( Z \) and  \( Y_{z} \) is the primary effect of interest. The value of  \( Y_{z} \) will increase by  \( T \) if  \( Z=1 \) relative to its value if  \( Z=0 \). Further, both  \( Y_{z} \) and  \( Z \) are causally related to the covariate  \( X. \)  IE, the treatment status and the outcome are determined by a function which depends only on  \( X \) and noise. As a result of this causal mechanism, it is easy to see that \( P \left( Z  \vert  Y_{1},~Y_{0} \right)   \neq  P \left( Z \right)  \) . So, the treatment status is not independent of the outcome as a result of the shared dependence on the value of  \( X \), and the two groups will have systematically different base outcomes. This simple example provides insight into a more general pattern: where treatment assignment depends on the same covariates as the outcome, there will be a systematic difference in outcome between the groups. We say that the covariates which affect both treatment assignment and outcome confound the estimation of causal effects - hence the name confounders.

\vspace{\baselineskip}

The explanation above uses an SCM to explain selection bias as it is understood in the Potential Outcomes framework. The same SCM can be used to explain the difference between predictive and causal inference from \textcite{Pearl2009CausalOverview}'s perspective. Predictive inference models the relationship between $Y$ and $Z$ directly. In some fixed set of observed data, the resultant correlation will include the effect caused by the direct influence of $Z$ on $Y$ as well as the \textit{backdoor} influence of $Z$ on $Y$, via $X$. If the data used to build the model, and future data to which it is applied, is independent and identically distributed, the conflation of these two paths of influence is irrelevant and prediction will be accurate. However, if there is an \textit{intervention}, which removes the \textit{backdoor} relationship between $X$ and $Z$, then the different paths of influence matter because only the direct path between $Z$ and $Y$ is relevant to predicting the outcome under intervention. This clarifies the difference between phenomenological (predictive) and fundamental (causal) correlation introduced in Chapter \ref{chap:intro}. Phenomenological correlations do not account for the mechanism of the relationship and, thus, do not extend to situations in which interventions occur. Fundamental, causal correlations capture the (still potentially stochastic) correlations between the components of the (true) mechanism and will hold under intervention.\par

\vspace{\baselineskip}

The model in Figure \ref{fig:confounder-scm} also hints at a viable path for counteracting selection bias (or uncovering fundamental, causal correlation if you prefer the SCM framework). If one conditions on X, then for fixed values of X, the only correlation left between  \( Y_{z} \) and  \( Z \)  is due to the edge between them. If we treat  \( X_{i} \) as an observation of a full set of covariates that fully identify a unit, this process is intuitively clear. By conditioning on the covariates, the correlation between  \( Y_{z} \) and  \( Z \) is estimated on subpopulations of identical units. Any change in outcome in these subpopulations must occur as a result of the change in the treatment status. But note that this only holds if we observe all of the confounding covariates related to both the treatment assignment and outcome.\par


\vspace{\baselineskip}

The process outlined here, and the assumptions required to operationalize it on real data, are outlined in the next section.\par

\section{Causal Inference as Predictive Inference under Assumptions}

\vspace{\baselineskip}
The ideas expressed above can be formalized in the Potential Outcome framework. The only nuance is that, under this framework, the outcomes under treatment and control are fixed, and therefore there is no directed edge, in the language of SCMs, between treatment assignment and outcome. Despite this small difference, statistical dependency between the potential outcomes and treatment assignment is induced by confounders exactly as above.\par


\vspace{\baselineskip}
Returning to the formalism of the estimands defined above, recall that finding the PATE requires estimating  \( E \left[ Y_{1} \right]  \) and  \( E \left[ Y_{0} \right]  \)  but that, in an observational setting, one only has access to  \( E \left[ Y \vert  Z = 1 \right]  \)  and  \( E \left[ Y \vert  Z=0  \right]  \) . If one assumes that the covariate vector  \( X_{i} \) contains all confounders we have the following:\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( E \left[ Y_{1} \right]  = E_{x~} \left[  E_{Y_{1}} \left[ Y_{1}  \vert  X \right]   \right]  = E_{x} \left[ E_{Y_{1}} \left[ Y_{1}  \vert  X, Z=1 \right]  \right]  =E_{x} \left[ E_{Y_{1}} \left[ Y_{}  \vert  X, Z=1 \right]  \right]  \) \par

\end{adjustwidth}

\begin{adjustwidth}{0.5in}{0.0in}
 \( E \left[ Y_{0} \right]  = E_{x~} \left[  E_{Y_{0}} \left[ Y_{0}  \vert  X \right]   \right]  = E_{x} \left[ E_{Y_{0}} \left[ Y_{0}  \vert  X, Z=0 \right]  \right]  =E_{x} \left[ E_{Y_{0}} \left[ Y_{}  \vert  X, Z=0 \right]  \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
This result implies that:\par


\vspace{\baselineskip}
 \( P \left( Y_{1}, Y_{0}  \vert  X, Z  \right) =P \left( Y_{1}, Y_{0} \vert  X \right)  =   \) \par


\vspace{\baselineskip}
This is the statement that the potential outcomes are conditionally independent of the treatment status given the covariates:\par

 \( Y_{1},~Y_{0} ~ \bigCI Z  \vert  X \) \par

And this is exactly what was demonstrated to be true using the SCM above under the assumption that  \( X \) does indeed include all confounders. Note that, under this framing, the problem of observational causal inference is reduced to a standard conditional estimation task  \( E \left[ Y_{z}  \vert  X, Z=z \right]  \) for the two treatment groups ( \( z~ \in  \{ 0, 1 \}  \) ). This is discussed in detail shortly.\par


\vspace{\baselineskip}
Formally, the Rubin Causal Model makes four assumptions to recover an unbiased estimate of causal effects:\par


\vspace{\baselineskip}
\begin{itemize}
    \item \textbf{Assumption 1 - Stable Unit Treatment Value Assumption:}
\end{itemize}\par

\begin{adjustwidth}{0.5in}{0.0in}
 \( Y \left( u \right)  =Z_{i} \times Y_{1} \left( u \right) + \left( 1-Z_{i} \right)  \times Y_{0} \left( u \right)  \) . IE, each unit has a fixed outcome under treatment and control and there is no interaction between the outcomes of different units.\par

\end{adjustwidth}


\vspace{\baselineskip}
\begin{itemize}
    \item \textbf{Assumption 2 - Exogeneity of Covariates:}
\end{itemize}\par

\begin{adjustwidth}{0.5in}{0.0in}
\( X_{i~}  \vert  Z_{i}=1 \) \textbf{ }=  \( X_{i}  \vert ~Z_{i}=0 \) . IE, covariates are independent of the treatment assignment.\par
\end{adjustwidth}

\vspace{\baselineskip}
\begin{itemize}
    \item \textbf{Assumption 3 - Conditional Independence:}
\end{itemize}\par

\begin{adjustwidth}{0.5in}{0.0in}
 \( Y_{1},~Y_{0} ~ \bigCI Z  \vert  X \) . IE, the covariate observation  \( X \) contains all confounders and there is conditional independence between outcomes and treatment assignment conditional on  \( X \) .\par

\end{adjustwidth}


\vspace{\baselineskip}
\begin{itemize}
    \item \textbf{Assumption 4 - Common Support:}
\end{itemize}\par

\begin{adjustwidth}{0.5in}{0.0in}
 \( 0~< P \left( Z=1   \vert  X = x \right)  < 1 \) for all  \( x~ \in support \left( X \right)  \) . IE, there is some probability that all possible units are exposed to both treatment and control. Philosophically, this is required for the existence of  \( Y_{1} \) and  \( Y_{0} \) because, in the absence of the potential for exposure, it is unclear what the (impossible) potential outcome would mean. Practically, this ensures that the outcome estimands in each group have common support and can be combined and averaged without inducing bias. This assumption can be relaxed, but this must be done with extreme care.\par

\end{adjustwidth}


\vspace{\baselineskip}
Under these assumptions, it is possible to estimate  \( E \left[ Y_{z}  \vert  X \right]  \) using  \( E \left[ Y  \vert  X, Z=z \right]  \) . So, the estimands defined above become:\par


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( CATE \left( X=x \right)  = E \left[ Y  \vert  X=x, Z=1 \right]  - E \left[ Y  \vert  X=x, Z=0 \right]  \) \par

\end{adjustwidth}


\vspace{\baselineskip}
\begin{adjustwidth}{0.5in}{0.0in}
 \( PATE =  \sum _{x}^{}P \left( X=x \right)   \times  \left[  E \left[ Y  \vert  X=x, Z=1 \right]  - E \left[ Y  \vert  X=x, Z=0 \right]   \right] _{} \) \par

\end{adjustwidth}


\vspace{\baselineskip}
These expressions make it clear that, under certain assumptions, observational causal inference reduces to estimation tasks, which are typically \textit{predictive }in that they rely only on standard statistical correlation. In the framing above, the first step of causal inference is to estimate the quantity \( E \left[ Y  \vert  X, Z \right]  \)  - the \textit{response surface} in the words of \textcite{Hill2011BayesianInference} - given the observed data from the two treatment groups. This is a standard predictive estimand. Under causal assumptions, the resultant estimated response surface(s) can then be combined to find average/conditional treatment effects.\par


\vspace{\baselineskip}
While this two-step process does clarify the connection between causal and predictive inference, it is a severe oversimplification. Different inference methods can be used to operationalize the ‘standard’ inference of the response surface - ranging from simple means across covariates blocks to fully-fledged function fitting. Moreover, one can define equally unbiased expressions for the estimands above, which depend exclusively on ‘standard’ inference but have meaningfully different mathematical forms. The different estimators implied by these two complications may be more or less efficient - producing ‘better’ results on the same dataset - and they may exhibit different performance under different distributional settings. Understanding these differences is crucial - the goal of research into causal inference estimators is to produce methods that are more efficient and work in specific, or across many, distributional settings. The next chapter explores different causal inference estimators and how their performance is affected by (a formalized notion of) the distributional setting of observed data. The formalization of distributional setting is particularly important as this is the constant background against which new estimators must be validated.\par

\end{document}