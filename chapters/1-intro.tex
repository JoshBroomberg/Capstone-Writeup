\documentclass[../main.tex]{subfiles}
\begin{document}

The last decade has seen enormous progress in the development of advanced algorithms that extract useful information from rapidly expanding stores of data \parencite{Lecun2015DeepLearning}. These algorithms - referred to as methods of machine learning - broadly operate in the paradigm of predictive inference: implicitly or explicitly learning a joint probability distribution over independent and identically distributed (IID) data and using the implied correlations to make conditional predictions \parencite{Scholkopf2019CausalityLearning}. As a result of this reliance on statistical correlation to infer the relationship between variables, these methods are useful but inherently limited. As is pointed out in \textcite{Pearl2009CausalOverview}, the accuracy of predictive methods fail when, rather than observing and predicting IID from outside of a static data generating process, the target of prediction is an (intended) intervention that changes the data generating mechanism and, as a result, the observed data distribution. Here is a simple example of this failure: In an observed data set, the presence of rain and a wet roof are correlated. Knowing the roof is wet will reduce one’s uncertainty about the weather. But, understanding that actively wetting the roof will not result in changes to the weather - despite the high \textit{predictive} power of a wet roof for the presence of rain - requires understanding that rain \textit{causes} the wet roof and not the reverse. This pattern holds in less contrived settings. If a new medicine tends to be prescribed to healthier (less at risk) patients during a trial period, then there will be a strong correlation between the new medicine and quick recovery regardless of the efficacy of the medicine. Given this correlation, predictive methods would (correctly) infer that prescribing the medicine increases the probability of recovery (\textit{within the IID observed data)}. But, upon intervening by giving the medicine to less healthy patients, one would find no positive effect - the predicted recover was the result of the previous treatment policy, not the medicine itself. These help establish an important truth: predicting the result of an intervention that changes some part of an observed system requires information beyond pure statistical correlation. I will show below that correlation does, in fact, play an important role in inferring the outcome of intervention but must be augmented.\par


\vspace{\baselineskip}
The challenge presented by this limitation is that intervention, of one kind or another, lies at the heart of many of the most important questions we would like to answer with growing stores of data. Social, economic and health policy is designed to \textit{change} systems to produce better outcomes for members of society. Companies take actions that \textit{change} market conditions to produce commercially-desirable outcomes. The intention to intervene means that predictive inference, applied to observed examples of different policies/actions, cannot be used to guide choices of which policy/action is best. Rather, what is required is methods of \textit{causal inference}. These methods can be thought of as uncovering correlations at the fundamental, mechanistic level rather than the correlations which appear at the (arbitrary) phenomenological level. These fundamental correlations persist under interventions that change the observed setting and, therefore, allow for the evaluation of the \textit{true, }causal\textit{ }effect of that intervention \parencite{Scholkopf2019CausalityLearning}.\par


\vspace{\baselineskip}
Historically, randomized experiments have been the gold standard in uncovering these fundamental, mechanistic correlations \parencite{Meldrum2000AStandard}. Randomizing the ‘intervention’ to which some unit is exposed, means that any observed (average) difference in outcome can only and will only reflect the effect of differential exposure to interventions and no other underlying cause. The problem with this approach is that running an experiment is often impossible (for nation-scale policy changes), expensive, slow, and quite possibly unethical (in cases where there is a reasonable expectation the intervention will mitigate suffering but to an unknown extent) \parencite{Meldrum2000AStandard}. Even if experimentation is viable, the validity of the effect recovered is limited to the subpopulation which is targeted for the experiment which, in many cases, will not be representative of the broader population and, further, is only correct on average but not for any single individual \parencite{Rothwell2006FactorsTrials}. For these reasons, there is great value in methods that are able to infer fundamental, mechanistic effects from non-experimental, non-randomized, \textit{observational} data at both the average and individual level. Methods that are capable of this inference are able to guide policy/action decisions based on the reservoirs of data already available to governments and corporations without the need for experimentation.\par


\vspace{\baselineskip}
The last few decades have seen the rapid development of just such methods of \textit{observational causal inference}. As a general rule, these methods build on existing tools of predictive inference but - through a combination of weak assumptions and modified estimands - recover fundamental mechanistic relations. These methods can be divided into two classes based on the nature of the targeted estimand. First, there is a mature literature on the estimation of \textit{average causal effects. }Methods in this class estimate the average effect of an intervention by undoing the \textit{confounding bias} in (average) group outcomes induced by factors that affect both the outcome of some intervention and the likelihood of receiving it (as in the medical treatment example above). This estimand is useful for government policy evaluation which focuses on the macro-efficacy of some intervention. Common approaches include inverse probability weighting \parencite{Horvitz1952AUniverse, Hirano2003EfficientScore} and matching estimators \parentcite{Rubin1974EstimatingStudies, Rosenbaum1983TheEffects}). See \textcite{Imbens2009RecentEvaluation} and \textcite{Athey2017TheEvaluation} for extensive reviews of the numerous methods which target average causal effects. The second class of methods is comprised of a newer body of work focused on estimating \textit{individual causal effects }-\ the effect of an intervention on specific individuals with the possibility of heterogeneous effects across the population of individuals. Estimation of individual effects requires disambiguating the confounding bias, as defined above, which operates at the level of intervention groups, from the heterogeneity of individuals’ responses to intervention at the sub-group level. The individual effect estimate is important where there is known heterogeneity in the response to intervention and the impact on individuals is relevant to policy decisions. For example, medical treatment often involves choices between hundreds of available options with highly varied individual responses and, as such, requires an understanding of how each treatment will affect a specific individual \parencite{Lu2018EstimatingMethods}. As a result of the inferential challenge implied by both confounding bias and heterogeneous response, methods for individual effect estimation tend to be newer and use modern, flexible semi/nonparametric estimators that require substantial computational resources. These include: regression trees \parencite{Su2009SubgroupTsai, Athey2016RecursiveEffects}, Random Forests \parencite{Wager2018EstimationForests, Athey2019GeneralizedForests}, Least Absolute Shrinkage and Selection Operator (Lasso) \parencite{Qian2011PerformanceRules, Tian2014ACovariates, Chen2017AScoring}, Neural Networks  \parencite{Johansson2016LearningInference, Johansson2018LearningDesigns, Schwab2018PerfectNetworks, Li2017MatchingEstimation, Kunzel2018TransferNetworks} or Bayesian machine learning \parencite{Hill2011BayesianInference, Taddy2016AExperimentation}.


\vspace{\baselineskip}

The diversity in methods, both for average and individual effect estimation, is to be expected. Causal inference fundamentally rests on the same inferential mechanics as predictive inference: accurately estimating distributions, or expectations over those distributions, from observed data. This presence of predictive inference is clear in both of the dominant frameworks for causal inference: the Rubin-Neyman Potential Outcomes Framework \parencite{Holland1986StatisticsInference} and Pearl’s Structural Causal Modelling (SCM) \parencite{Pearl2009CausalOverview}. Causal inference is, however, distinguished from purely predictive inference by the addition of mechanisms and assumptions to counteract the confounding bias induced by the shift in observational setting created by a (potential) intervention. These mechanisms and assumptions are simply combined with the more fundamental inference tasks rather than replacing them. This framing of causal inference methods, which will be formalized in the chapters below, provides an intuitive justification for the existence of many equivalent estimators for the average and individual effect estimands. There are two distinct sources of equivalency evident in the framing above. First, for any given mechanism (and set of assumptions) for reversing observational bias, there tend to be many equivalent predictive inference methods that can then be used to operationalize the estimation of the target effect. Second, for any given data set, there are usually multiple equivalent mechanisms (and requisite assumptions) that can be used to mitigate confounding bias. Much like in pure predictive inference, different, equivalent inference techniques and bias-mitigation mechanisms may exhibit very different performance and properties in finite samples while all converging to the same estimands in the asymptotic limit.\par


\vspace{\baselineskip}
This raises the challenge that is the core focus of this paper. Given numerous equivalent methods for observational causal inference, how does one know which one is best? This question is crucially important - both for researchers who are working to improve these methods and for practitioners who wish to apply the best method available to make important policy decisions in their fields. It is also a question that is hard to answer. Unlike in predictive inference, the ground-truth is not present in the training data. Under the Potential Outcomes framework, this stems from the fact that we never see the same unit under different interventions and thus can never know the true causal effect of the intervention on that unit. Equivalently, in Pearl’s SCMs, the observed data is distributed differently to the data under intervention. In either case, the result is the same - the training data does not contain the ground-truth. Further, while some methods may lend themselves to formal, asymptotic convergence proofs, these are not universally available, rely on assumptions which often render them mutually incomparable between methods and, importantly, do not apply to performance in the finite data regime which is of the most relevance for method selection \parencite{Knaus2018MachineEvidence}. This situation is further complicated by a large space of different causal inference challenges corresponding to different distributional settings in the observed data. It will be demonstrated that inference methods are likely to perform quite differently on finite samples corresponding to different locations in the \textit{problem space }of different distributional settings. This implies there may not be one method of causal inference which is universally superior across all finite data samples. So, in short, answering the question of which causal inference algorithm is best is hard because it requires finite sample evaluation, without access to ground-truth, and with the challenge of heterogeneous performance under different distributional settings. This paper tackles this challenge by proposing an evaluation method that allows for finite sample evaluation of arbitrary causal inference methods across an arbitrary selection of distributional settings.\par


\vspace{\baselineskip}
Researchers have typically addressed the evaluation problem outlined above through two complementary evaluation methods, both of which provide access to a ground-truth causal effect with some trade-off in evaluative efficacy. Empirical evaluation methods use (randomized) experimental datasets and test an estimator’s ability to reproduce the experimental result when using a non-randomized control group (to simulate an observational setting). This strategy was pioneered by \textcite{Lalonde1986EvaluatingData} with iterative development by \textcite{Heckman1998MatchingEstimator}, \textcite{Dehejia1999CausalPrograms}, \textcite{Dehejia2002PropensityStudies}, \textcite{Dehejia2005PracticalTodd}, and \textcite{Smith2005DoesEstimators} with notable contributions from \textcite{Hill2005AAnalyses} and \textcite{Shadish2008CanAssignments}. The data used in these evaluations is realistic - in so far as it is drawn from data collected as part of formal, real-world studies - but only a small number of such experimental datasets (with appropriate observational controls) are available and these cover a small and poorly defined subset of the problem space\footnote{ The true data generating process in these datasets is unknown which means it is unclear what distribution properties the data displays. }. In contrast, the second approach, synthetic evaluation, relies on synthetic data generated by a hand-crafted data generating process (DGP). These DGPs can simulate any location in the distributional problem space but are often unrealistic in terms of the number and type of variables used as well as the functional forms used to simulate the outcome and intervention data. The mechanisms behind these two approaches - and the strengths and weaknesses that these mechanisms imply - are formalized in the chapters below.\par


\vspace{\baselineskip}
The primary contribution of this paper is a hybrid evaluation method designed to overcome the weaknesses of both of the evaluation methods above. The method proposed is based on the \textit{sampling} of synthetic DGPs defined over real, observational data. This hybrid approach combines the diagnostic clarity of synthetic evaluation methods with the realistic distributions of empirical evaluation methods. Appropriate parameterization of the sampling process allows for the generation of DGPs which are in well-defined, specific locations in problem space without the problematic implications of hand-crafted, ‘targeted’ (but potentially biased) design. Additionally, the sampling approach also allows for the generation of many distinct instances of evaluative datasets in the same problem class, allowing for repeat evaluation of a method and, thus, for convergence to an accurate distribution of performance - revealing average/best/worst case results rather than just single-point samples from an unknown distribution.\par


\vspace{\baselineskip}
The hybrid approach proposed is not entirely novel. A number of other authors have proposed similar methods albeit with small but significant idiosyncrasies. These methods are reviewed in Chapter \ref{chap:litreview} below. A common shortcoming of all the existing work in this subfield is the lack of tooling to make the proposed evaluation method available to others such that it can be applied to methods not covered in the original papers and used with base data and problem-instance settings relevant to different academic fields of study. Existing code, if available, is tightly coupled to specific empirical source data and inflexibly/confusingly parameterized. With this context in mind, this paper makes two contributions. First, a thorough comparative analysis of the hybrid approaches proposed in the literature with the goal of selecting a single, strong hybrid evaluation method (or a synthesis of multiple methods). And, second, an accompanying tool that makes it easy to apply the selected method of hybrid evaluation to arbitrary causal inference methods using arbitrary base data and arbitrary problem-instance settings. The goal of this tool is to provide a consistent, universal means by which causal inference researchers - and users - can develop, compare and select methods of causal inference.\par


\vspace{\baselineskip}
The rest of this paper proceeds as follows: Chapter \ref{chap:framework} introduces causal inference and formalizes the notation and terminology used throughout this work. Chapter \ref{chap:problemspace} establishes the causal inference problem space - the space of distributional settings that impact the performance of different causal inference estimators. This is the space over which estimators should be evaluated. Chapter \ref{chap:litreview} presents a review of the literature on causal estimator evaluation, comparing asymptotic, Monte Carlo and Hybrid Monte Carlo evaluation methods. Chapters \ref{chap:macdesign} and \ref{chap:macimplementation} introduce Maccabee - a package for Hybrid Monte Carlo Benchmarking. Chapter \ref{chap:macdesign} presents the design of the Maccabee benchmark, which builds on the best method from the review in Chapter \ref{chap:litreview}. Chapter \ref{chap:macimplementation} explains the implementation used to operationalize this design. Chapter \ref{chap:macvalidation} present preliminary validation results. Chapter \ref{chap:conclusion} concludes.\par

\end{document}
